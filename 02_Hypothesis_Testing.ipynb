{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "* [Hypothesis Testing](#hypothesis_testing)\n",
    "    * [Steps of Hypothesis Testing](#steps_hypothesis_testing)\n",
    "    * [p-value](#p_value)\n",
    "    * [Type I error - false positive](#type_1_error)\n",
    "    * [Type II error - false negative](#type_2_error)\n",
    "    * [Power of a Test](#power_of_test)\n",
    "* [Tests of means of numerical data](#tests_numerical_data)\n",
    "    * [Example 1: One-sample Student's t-test](#one_sample_t_test)\n",
    "    * [Example 2: Two-sample Student's t-test](#two_sample_t_test)\n",
    "    * [Example 3: One-way ANOVA (balanced)](#one_way_anova)\n",
    "    * [Example 4: Two-way ANOVA (balanced, with interactions)](#two_way_anova)\n",
    "* [Tests of frequencies of categorical data](#tests_categorical_data)\n",
    "    * [Example 1: One-way Pearson Chi-Square test - goodness of fit](#one_way_chi2)\n",
    "    * [Example 2: Two-way Pearson Chi-Square test - test of independence](#two_way_chi2_independence)\n",
    "    * [Example 3: Two-way Pearson Chi-Square test - test for homogeneity](#two_way_chi2_homogeneity)\n",
    "* [Statistical modeling for hypothesis testing](#statistical_modeling_for_hypothesis_testing)\n",
    "    * [Example 1: Linear Regression for Two-sample Student's t-test](#linear_regression_t_test)\n",
    "    * [Example X: Generalised Linear Model Hypothesis Testing: TODO](#glm_t_test)\n",
    "    * [Example XX: Linear Regression for One-way ANOVA: TODO](#linear_regression_anova)\n",
    "* [Confidence Interval](#confidence_interval)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis Testing <a class=\"anchor\" id=\"hypothesis_testing\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In **Frequentist approach**, we use maximum likelihood estimation (MLE) as a method of estimating the parameters of a probability distribution by maximizing a likelihood function, so that under the assumed statistical model the observed data is most probable. The point in the parameter space that maximizes the likelihood function is called the maximum likelihood estimate.\n",
    "\n",
    "Frequentists don’t treat the uncertainty in the true parameter value probabilistically. However, they handle uncertainty by limiting the long-term error rates, either by **comparing the estimated parameter against a null value (NHST)**, or by calculating **confidence intervals**.\n",
    "\n",
    "wiki - maximum likelihood\\\n",
    "https://www.probabilisticworld.com/frequentist-bayesian-approaches-inferential-statistics/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps of Hypothesis Testing <a class=\"anchor\" id=\"steps_hypothesis_testing\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ol>\n",
    "<li> A <b>random sample</b> is drawn from the population.</li>\n",
    "<li> A <b>null hypothesis</b> is formulated for the population (ex. there is null difference between the population mean and the value 110).</li>\n",
    "<li> A <b>test-statistic</b> is calculated, of which we know the probability distribution. A test-statistic is a quantity derived from the sample, a numerical summary of a data-set that reduces the data to one value. An important property of a test statistic is that <b>its sampling distribution under the null hypothesis must be calculable</b>. If an arbitrarily large number of samples, each involving multiple observations (data points), were separately used in order to compute one value of a statistic (such as, for example, the sample mean or sample variance) for each sample, then the sampling distribution is the probability distribution of the values that the statistic takes on. In many contexts, only one sample is observed, but the sampling distribution can be found theoretically. Note, the test-statistics will have a dependency on the sample size (abm).</li>\n",
    "<li> <b>Comparing the observed value of the statistics with the corresponding distribution</b>, we can find the likelihood that a value as extreme as or more than the observed one is found by chance, if we assume the null hypothesis as true. This is the <b>p-value</b>.</li>\n",
    "    <li> If <b>$p <\\alpha $ </b> (where $\\alpha$ is the <b> significance level </b>, typically $\\alpha = 0.05$), we <b>reject the null hypothesis</b> and speak of a statistically significant difference. </li>\n",
    "</ol>    \n",
    "\n",
    "Hypothesis testing aims to **reduce the long-term error rates**: if I repeat the experiment many times with many different random samples (infinite times), only $\\alpha$% of times I will wrongly reject the null hypothesis. \n",
    "\n",
    "*Book: An Introduction to Statistics with Python*\\\n",
    "*wiki - Test statistic \\\n",
    "*wiki - Sampling distribution \\\n",
    "https://www.probabilisticworld.com/frequentist-bayesian-approaches-inferential-statistics/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### p-value <a class=\"anchor\" id=\"p_value\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A value of $p < \\alpha$ for the null hypothesis has to be interpreted as follows: If the null hypothesis is true, **the chance to find a test statistic as extreme as or more extreme than the one observed is less than $\\alpha$**. Typically $\\alpha$ is set to 0.05, which means that if the null hypothesis is true, the chance to find a test statistic as extreme as or more extreme than the one observed is less than 5%.\\\n",
    "This is not the same as saying that the null hypothesis is false, and even less so that an alternative hypothesis is true!\n",
    "\n",
    "Book: An Introduction to Statistics with Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered S3 methods overwritten by 'ggplot2':\n",
      "  method         from \n",
      "  [.quosures     rlang\n",
      "  c.quosures     rlang\n",
      "  print.quosures rlang\n",
      "Warning message in is.na(x):\n",
      "\"is.na() aplicado a un objeto que no es (lista o vector) de tipo 'expression\"Warning message in is.na(x):\n",
      "\"is.na() aplicado a un objeto que no es (lista o vector) de tipo 'expression\""
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeAAAAEsCAMAAAAsIJBoAAAAJFBMVEUAAAAAAP8zMzNNTU2k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\n9j9ouVhYAQAAAABJRU5ErkJggg==",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sigma <- 1\n",
    "mu <- 0\n",
    "bounds <- c(mu-3*sigma, mu+3*sigma)\n",
    "\n",
    "lower.alpha.x <- 1.5\n",
    "upper.alpha.x <- 4\n",
    "step <- (upper.alpha.x - lower.alpha.x) / 100\n",
    "cord.alpha.x <- c(lower.alpha.x,seq(lower.alpha.x,upper.alpha.x,step),upper.alpha.x)\n",
    "cord.alpha.y <- c(0,dnorm(seq(lower.alpha.x,upper.alpha.x,step),mu,sigma),0)\n",
    "poligono.alpha <- data.frame(cord.alpha.x,cord.alpha.y)\n",
    "\n",
    "lower.beta.x <- -4\n",
    "upper.beta.x <- lower.alpha.x\n",
    "step <- (upper.beta.x - lower.beta.x) / 100\n",
    "cord.beta.x <- c(lower.beta.x,seq(lower.beta.x,upper.beta.x,step),upper.beta.x)\n",
    "cord.beta.y <- c(0,dnorm(seq(lower.beta.x,upper.beta.x,step),mu+2,sigma),0)\n",
    "poligono.beta <- data.frame(cord.beta.x,cord.beta.y)\n",
    "\n",
    "# curve(dnorm(x,mu,sigma),xlim=bounds) \n",
    "# polygon(cord.x,cord.y,col='skyblue')\n",
    "\n",
    "library(ggplot2)\n",
    "options(repr.plot.width=4, repr.plot.height=2.5)\n",
    "ggplot(data.frame(x = c(-4, 6)), aes(x)) + \n",
    "  stat_function(fun = dnorm, args = list(mean = mu, sd = sigma), col='red') +\n",
    "  stat_function(fun = dnorm, args = list(mean = mu+2, sd = sigma), col='blue') +\n",
    "  geom_polygon(data=poligono.alpha, mapping=aes(x=cord.alpha.x, y=cord.alpha.y),fill='red', alpha=0.3) +\n",
    "  geom_polygon(data=poligono.beta, mapping=aes(x=cord.beta.x, y=cord.beta.y),fill='blue', alpha=0.3) +\n",
    "  annotate(\"text\", label = expression(alpha), x = 2, y = 0.03, size = 3, colour = \"red\") +\n",
    "  annotate(\"text\", label = expression(beta), x = 1, y = 0.03, size = 3, colour = \"blue\") + \n",
    "  annotate(\"text\", label = \"H0\", x = 0, y = 0.35, size = 3) +\n",
    "  annotate(\"text\", label = \"HA\", x = 2, y = 0.35, size = 3) + xlab(\"\") + ylab(\"\") \n",
    "#   stat_function(fun = dnorm, args = list(mean = mu+1, sd = sigma, col='blue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Type I error - false positive <a class=\"anchor\" id=\"type_1_error\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wrongly reject the null hypothesis. The null hypothesis is true, but we reject it.\\\n",
    "We set a threshold $p < \\alpha$ for rejecting the null hypothesis, where **$\\alpha$ is called the significance level**. Therefore **there is a $\\alpha$ probability of wrongly rejecting it (abm)**.\n",
    "If we assign the value of $\\alpha =0.05$, there is a 5% probability of wrongly rejecting it (abm).\n",
    "\n",
    "wiki - Type I and type II errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Type II error - false negative <a class=\"anchor\" id=\"type_2_error\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't reject the null hypothesis even if we should reject it. The null hypothesis is false but we find $p > \\alpha$. \\\n",
    "**Given an alternative hypothesis $H_{A}$, if we assume that this is the true hypothesis, the probability of rejecting it even if it is true is $\\beta$.** \\\n",
    "The chance of correctly rejecting the null hypothesis (and accept the alternative one) is $1 - \\beta$ (**power of the test**)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Power of a Test <a class=\"anchor\" id=\"power_of_test\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **power o sensitivity** of a hypothesis test is the **probability that the test rejects the null hypothesis $H_{0}$ when a specific alternative hypothesis $H_{A}$ is true**. To estimate this value, **we need to have defined an alternative hypothesis $H_{A}$**.\n",
    "\n",
    "Given an alternative hypothesis $H_{A}$:\n",
    "- $\\beta*100$ is the chance of wrongly accepting the null hypothesis.\n",
    "- $(1-\\beta)*100$ is the chance of correctly rejecting the null hypothesis. This is the **power** of the test.\n",
    "\n",
    "Book: An Introduction to Statistics with Python \\\n",
    "wiki - Power of a test \\\n",
    "Check here for some examples:\\\n",
    "https://cran.r-project.org/web/packages/pwr/vignettes/pwr-vignette.html \\\n",
    "https://www.r-bloggers.com/2017/07/power-analysis-and-sample-size-calculation-for-agriculture/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests of means of numerical data <a class=\"anchor\" id=\"tests_numerical_data\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we consider the most common hypotesis testing techniques that compare the **mean of a numerical variable across groups**. More specifically:\n",
    "\n",
    "<l>\n",
    "<li>Comparison of the mean for one group with a fixed value. </li>\n",
    "<li>Comparison of the means of two groups. </li>\n",
    "<li>Comparison of the means of three or more groups. </li>    \n",
    "</l>\n",
    "\n",
    "In case the data are normally distributed (or approximately) we can use the so-called *parametric tests*. If the data are not normally distributed, the corresponding *non-parametric test* should be used.  \n",
    "\n",
    "Book: An Introduction to Statistics with Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 1: One-sample Student's t-test <a class=\"anchor\" id=\"one_sample_t_test\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **one sample t-test** is a statistical procedure used to **determine whether a sample of observations could have been generated by a process with a specific mean $m_{0}$**. It assumes a population with normal distribution $N(\\mu, \\sigma)$.\n",
    "\n",
    "The alternative hypothesis assumes that some difference exists between the true mean ($\\mu$) and the comparison value ($m_{0}$), whereas the null hypothesis assumes that no difference exists.\n",
    "\n",
    "<ol>\n",
    "<li> We have a random sample of n elements $x_{i}$ taken from the population. </li>\n",
    "<li> Null hypothesis $H_{0}: \\mu=m_{0}$.</li>\n",
    "<li> From the observed sample data we calculate the test-statistic\n",
    "    $t = \\frac{\\bar{x}-m_{0}}{s/\\sqrt{n}}$, where $\\bar{x}$ is the sample mean and s is the sample standard deviation, \n",
    "    $\\bar{x}=\\frac{\\sum_{i}^{n}x_{i}}{n}$, $s^{2}=\\frac{\\sum_{i}^{n}(x_{i}- {{\\bar{x}}})^2}{n-1}$. If we assume the null hypothesis, i.e. the sample is randomly taken from the normal distribution $N(m_{0},\\sigma)$, then the statistic <b>t will follow a t-Student distribution with n-1 degrees of freedom</b>.   \n",
    "</li>\n",
    "<li> <b>Comparing the observed value of the statistics with the corresponding distribution</b>, we can find the likelihood that a value as extreme as or more than the observed one is found by chance. This is the <b>p-value</b>.</li>\n",
    "<li> If <b>p < 0.05</b> (or below a threshold we decide), we <b>reject the null hypothesis</b> and speak of a statistically significant difference. </li>\n",
    "</ol>  \n",
    "\n",
    "<br />\n",
    "The one sample t-test has four main assumptions:\n",
    "\n",
    "- The dependent variable must be continuous (interval/ratio).\n",
    "- The observations are independent of one another.\n",
    "- The dependent variable should be approximately normally distributed.\n",
    "- The dependent variable should not contain any outliers.\n",
    "\n",
    "wiki - Student's t-distribution\\\n",
    "Book: An Introduction to Statistics with Python\\\n",
    "https://www.statisticssolutions.com/manova-analysis-one-sample-t-test/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Numerical example**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You want to compare the notes of 10 students to the national average of 110.\\\n",
    "Question: is the mean values of the notes significally higher than the national average?\n",
    "\n",
    "Book: An Introduction to Statistics with Python, p.126"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Null hypothesis: there is no difference between the population mean $\\mu$ and the value 110.\\\n",
    "$H_{0}: \\mu = 110$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes <- c(109.4, 76.2, 128.7, 93.7, 85.6, 117.7, 117.2, 87.3, 100.3, 55.1)\n",
    "n <- length(notes)\n",
    "x_mean <- mean(notes)\n",
    "s2 <- sum((notes-x_mean)^2)/(n-1)\n",
    "t <- (x_mean-110)/(sqrt(s2)/sqrt(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "-1.83602499978154"
      ],
      "text/latex": [
       "-1.83602499978154"
      ],
      "text/markdown": [
       "-1.83602499978154"
      ],
      "text/plain": [
       "[1] -1.836025"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "t # t value measured from our sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_test <- t.test(notes, mu =110)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "\tOne Sample t-test\n",
       "\n",
       "data:  notes\n",
       "t = -1.836, df = 9, p-value = 0.09954\n",
       "alternative hypothesis: true mean is not equal to 110\n",
       "95 percent confidence interval:\n",
       "  81.25062 112.98938\n",
       "sample estimates:\n",
       "mean of x \n",
       "    97.12 \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong>t:</strong> -1.83602499978154"
      ],
      "text/latex": [
       "\\textbf{t:} -1.83602499978154"
      ],
      "text/markdown": [
       "**t:** -1.83602499978154"
      ],
      "text/plain": [
       "        t \n",
       "-1.836025 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "0.0995384365279066"
      ],
      "text/latex": [
       "0.0995384365279066"
      ],
      "text/markdown": [
       "0.0995384365279066"
      ],
      "text/plain": [
       "[1] 0.09953844"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "t_test\n",
    "t_test$statistic\n",
    "t_test$p.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probability of having obtained our random sample out of a normal population centered in 110 is 9.95%. Since this value is higher than our threshold of 5%, we cannot reject the null hypothesis. It means that there is no statistically significant difference between the real population mean and 110 or in other words the observed value of 97.1 is not significantly different from 110."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 2: Two-sample Student's t-test <a class=\"anchor\" id=\"two_sample_t_test\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **two sample Student's t-test** is a statistical procedure used to **compare two groups of observations**. The goal is to determine if both groups could have been generated by the **same process with the same mean**. It assumes populations with normal distribution $N(\\mu_{1}, \\sigma_{1})$ and $N(\\mu_{2}, \\sigma_{2})$.\n",
    "\n",
    "The alternative hypothesis assumes that some difference exists between the two mean values $\\mu_{1} \\neq \\mu_{2}$, whereas the null hypothesis assumes that no difference exists $\\mu_{1}=\\mu_{2}$.\n",
    "\n",
    "<ol>\n",
    "<li> We have two random samples: $n_{1}$ elements $x_{1i}$ taken from group 1 and $n_{2}$ elements $x_{2i}$ taken from group 2. </li>\n",
    "<li> Null hypothesis $H_{0}: \\mu_{1}=\\mu_{2}$.</li>\n",
    "<li> From the observed sample data we calculate the <b> test-statistic\n",
    "    $t = \\frac{\\bar{x}_{1}-\\bar{x}_{2}}{s_{\\bar{\\Delta}}}$ </b>, where $\\bar{x}_{1}$ is the sample mean of group 1, $\\bar{x}_{2}$ is the sample mean of group 2 and $s_{\\bar{\\Delta}}$ is the standard deviation of the difference between the samples means. Explicitely \n",
    "    $\\bar{x}_{1}=\\frac{\\sum_{i}^{n1}x_{1i}}{n1}$, $\\bar{x}_{2}=\\frac{\\sum_{i}^{n2}x_{2i}}{n2}$, $s_{\\bar{\\Delta}}^{2}=\\frac{s^{2}_{1}}{n1}+\\frac{s^{2}_{2}}{n2}$, where $s^{2}_{1}=\\frac{\\sum_{i}^{n1}(x_{1i}- {{\\bar{x}_{1}}})^2}{n1-1}$ and $s^{2}_{2}=\\frac{\\sum_{i}^{n2}(x_{2i}- {{\\bar{x}_{2}}})^2}{n2-1}$ are the variances of groups 1 and 2.\n",
    "    If we assume the null hypothesis, i.e. the two samples are randomly taken from normal distributions with the same mean, then the statistic <b>t can be approximated as an ordinary Student's t-distribution with the degrees of freedom calculated using: check Welch's t-test</b>.   \n",
    "</li>\n",
    "<li> <b>Comparing the observed value of the statistics with the corresponding distribution</b>, we can find the likelihood that a value as extreme as or more than the observed one is found by chance. This is the <b>p-value</b>.</li>\n",
    "<li> If <b>p < 0.05</b> (or below a threshold we decide), we <b>reject the null hypothesis</b> and speak of a statistically significant difference. </li>\n",
    "</ol>  \n",
    "\n",
    "<br />\n",
    "The two samples t-test has these main assumptions:\n",
    "\n",
    "- normally distributed\n",
    "- independent groups\n",
    "\n",
    "\n",
    "wiki - Student's t-test\\\n",
    "Book: An Introduction to Statistics with Python\\\n",
    "C.Sala Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Numerical Example**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to compare the performance measurements of a racing team on two occasions: race1 and race2.\\\n",
    "Question: is the preformance in race2 significantly better than in race1?\n",
    "\n",
    "Book: An Introduction to Statistics with Python, p.144"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "race1_scores <- c(79, 100, 93, 75, 84, 107, 66, 86, 103, 81, 83, 89, 105, 84, 86, 86, 112, 112, 100, 94)\n",
    "race2_scores <- c(92, 100, 76, 97, 72, 79, 94, 71, 84, 76, 82, 57, 67, 78, 94, 83, 85, 92, 76, 88)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2 <- var(race1_scores)/length(race1_scores)+var(race2_scores)/length(race2_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "t <- (mean(race1_scores)-mean(race2_scores))/sqrt(s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "2.43475609142344"
      ],
      "text/latex": [
       "2.43475609142344"
      ],
      "text/markdown": [
       "2.43475609142344"
      ],
      "text/plain": [
       "[1] 2.434756"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "\tWelch Two Sample t-test\n",
       "\n",
       "data:  race1_scores and race2_scores\n",
       "t = 2.4348, df = 37.3, p-value = 0.0198\n",
       "alternative hypothesis: true difference in means is not equal to 0\n",
       "95 percent confidence interval:\n",
       "  1.529079 16.670921\n",
       "sample estimates:\n",
       "mean of x mean of y \n",
       "    91.25     82.15 \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "t_test <- t.test(race1_scores, race2_scores)\n",
    "t_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probability that the two random samples are taken from normal populations centered around the same mean is 1.98%. Since this value is lower than our threshold of 5%, we can reject the null hypothesis. It means that there is statistically significant difference between the two groups populations means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "TRUE"
      ],
      "text/latex": [
       "TRUE"
      ],
      "text/markdown": [
       "TRUE"
      ],
      "text/plain": [
       "[1] TRUE"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "11.7912508945373"
      ],
      "text/latex": [
       "11.7912508945373"
      ],
      "text/markdown": [
       "11.7912508945373"
      ],
      "text/plain": [
       "[1] 11.79125"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "9.09999999999999"
      ],
      "text/latex": [
       "9.09999999999999"
      ],
      "text/markdown": [
       "9.09999999999999"
      ],
      "text/plain": [
       "[1] 9.1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n1 <- length(race1_scores)\n",
    "n2 <- length(race2_scores)\n",
    "n1 == n2\n",
    "sd1 <- sd(race1_scores)\n",
    "sd2 <- sd(race2_scores)\n",
    "sd <- mean(c(sd1,sd2))\n",
    "sd\n",
    "delta <- mean(race1_scores)-mean(race2_scores)\n",
    "delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "     Two-sample t test power calculation \n",
       "\n",
       "              n = 20\n",
       "          delta = 9.1\n",
       "             sd = 11.79125\n",
       "      sig.level = 0.05\n",
       "          power = 0.6622166\n",
       "    alternative = two.sided\n",
       "\n",
       "NOTE: n is number in *each* group\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# calculate power of the test\n",
    "power.t.test(n = n1, delta = delta, sd = sd, sig.level = 0.05,\n",
    "             power = NULL,\n",
    "             type = \"two.sample\",\n",
    "             alternative = \"two.sided\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a 66% chance of correctly rejecting the null hypothesis, assuming that the magnitude of the effect we observe is true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "     Two-sample t test power calculation \n",
       "\n",
       "              n = 27.35009\n",
       "          delta = 9.1\n",
       "             sd = 11.79125\n",
       "      sig.level = 0.05\n",
       "          power = 0.8\n",
       "    alternative = two.sided\n",
       "\n",
       "NOTE: n is number in *each* group\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "power.t.test(n = NULL, delta = delta, sd = sd, sig.level = 0.05,\n",
    "             power = 0.8,\n",
    "             type = \"two.sample\",\n",
    "             alternative = \"two.sided\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would need 27 observations per group to correctly reject the null hypothesis with a 80% probability (assuming the effect size we observe is true)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "     Two-sample t test power calculation \n",
       "\n",
       "              n = 20\n",
       "              d = 0.7717587\n",
       "      sig.level = 0.05\n",
       "          power = 0.6622237\n",
       "    alternative = two.sided\n",
       "\n",
       "NOTE: n is number in *each* group\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Alternative library for calculating power of t test\n",
    "library(pwr)\n",
    "\n",
    "## Cohen effect size d\n",
    "d <- delta/sd\n",
    "\n",
    "pwr.t.test(d = d, n = n1, sig.level = 0.05)\n",
    "\n",
    "## https://cran.r-project.org/web/packages/pwr/vignettes/pwr-vignette.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 3: One-way ANOVA (balanced) <a class=\"anchor\" id=\"one_way_anova\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANOVA (Analysis of Variance)** is a statistical test used to **analyze the difference between the means of more than two groups**.\\\n",
    "The **one-way ANOVA** is a statistical procedure used to assess whether the means of a quantitative variable within several pre-defined groups differ from each other. It assumes that all samples are drawn from **normally distributed populations with equal variances**, $N(\\mu_{i}, \\sigma)$ . Also it's quite important to have exactly the **same number of samples in each analysis group (balanced ANOVA)**.\\\n",
    "The name **one-way** refers to the fact that we are considering only one variable that distinguish between different groups: this variable is usually called *factor* or *treatment*. If the test is including also another variable, it is reffered as *two-way ANOVA*. For example, if we compare a group with No treatment, another with treatment A and a third one with treatment B, we will perform one-way ANOVA. If we want to distinguish between man and women (factors: treatment and gender) we will perform a two-way ANOVA.\\\n",
    "For comparison between two groups the t-test leads to exactly the same result as the one-way ANOVA.\n",
    "\n",
    "The one-way ANOVA can be used to assess whether any of the treatments is on average superior, or inferior, to the others versus the null hypothesis that all treatments yield the same mean response. This is an **example of an *omnibus* test, meaning that a single test is performed to detect any of several possible differences**. Alternatively, we could carry out pairwise tests among the treatments (for instance, in the medical trial example with four treatments we could carry out six tests among pairs of treatments). The disadvantage of the one-way ANOVA is that if we reject the null hypothesis, we do not know which treatments can be said to be significantly different from the others.\n",
    "\n",
    "The alternative hypothesis assumes that some difference exists between the mean values of the groups, whereas the null hypothesis assumes that no difference exists.\n",
    "\n",
    "<ol>\n",
    "<li> We have $n_{groups}$ random samples each one with the same number $n$ of elements (<i>balanced</i> ANOVA). </li>\n",
    "<li> Null hypothesis $H_{0}: \\mu_{1}=\\mu_{2}= ...=\\mu_{n_{groups}}$.</li>\n",
    "<li> From the observed samples data we calculate the <b> F-test statistic\n",
    "    $F = \\frac{variance-between-treatments}{variance-within-treatments} =\\frac{SS_{treatments}/DF_{treatments}}{SS_{error}/DF_{errors}}=\\frac{MS_{treatments}}{MS_{error}} = \\frac{SS_{treatments}/(n_{groups}-1)}{SS_{error}/(n_{total}-n_{groups})}$ </b>, where the summation is called the sum of squares (SS), the divisor is called the degrees of freedom (DF) and the result is called the mean square (MS). These values are related by the following equivalences: $SS_{total} = SS_{error} + SS_{treatments}$ and $DF_{total} = DF_{error}+DF_{treatments}$.<br>\n",
    "Explicitely, $SS_{total} =\\sum_{j}^{n_{groups}}\\sum_{i}^{n}(x_{ij}- {{\\bar{x}_{total}}})^2 $ is the total sum of squares based on all the observation deviations from the grand mean, $SS_{error} =\\sum_{j}^{n_{groups}}\\sum_{i}^{n}(x_{ij}- {{\\bar{x}_{j}}})^2 $ is based on all the observation deviations from their group mean, $SS_{treatments} =n\\sum_{j}^{n_{groups}}(\\bar{x}_{j}-\\bar{x}_{total})^2 $ is based on the deviations of treatment means from the grand mean, the result being multipied by the number n of observations in each treatment to account for the difference between the variance of observations and variance of means. Regarding the degrees of freedom, they follow the equations: $DF_{total}= n_{total}-1$, $DF_{treatments}= n_{groups}-1$ and $DF_{error}= n_{total}-n_{groups}$. <br>\n",
    "<b>The ANOVA test is based on a comparison of the observed variation between the groups with the observed variability within the groups.</b><br> \n",
    "If we assume the null hypothesis ($H_{0}: \\mu_{1}=\\mu_{2}= ...=\\mu_{n_{groups}}$), then the statistic <b>F will follow a F-distribution</b>.   \n",
    "</li>\n",
    "<li> <b>Comparing the observed value of the statistics with the corresponding distribution</b>, we can find the likelihood that a value as extreme as or more than the observed one is found by chance. This is the <b>p-value</b>.</li>\n",
    "<li> If <b>p < 0.05</b> (or below a threshold we decide), we <b>reject the null hypothesis</b> and speak of a statistically significant difference. </li>\n",
    "</ol>  \n",
    "\n",
    "<br />\n",
    "The one-way ANOVA stands on these main assumptions:\n",
    "\n",
    "- independence of observations.\n",
    "- normality – the distributions of the residuals are normal.\n",
    "- equality (or \"homogeneity\") of variances, called homoscedasticity — the variance of data in groups should be the same.\n",
    "- same number of samples in each analysis group (*balanced* ANOVA).\n",
    "- no significant outliers in any cell of the design.\n",
    "\n",
    "Book: An Introduction to Statistics with Python\\\n",
    "wiki - Analysis of variance\\\n",
    "wiki - F-test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Numerical Example**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the built-in R data set named PlantGrowth. It contains the weight of plants obtained under a control and two different treatment conditions. It consists of 30 observations, 10 per group (control, treatment 1 and treatment 2).\n",
    "\n",
    "Question: is there significant difference in the mean of the three groups?\\\n",
    "Here we assume that we have done all the necessary tests for checking normality and homoscedasticity conditions.\n",
    "\n",
    "https://www.datanovia.com/en/lessons/anova-in-r/ \\\n",
    "https://www.thoughtco.com/example-of-an-anova-calculation-3126404"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(tidyverse)\n",
    "library(dplyr)\n",
    "library(ggpubr)\n",
    "library(rstatix)\n",
    "data(\"PlantGrowth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>30</li>\n",
       "\t<li>2</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 30\n",
       "\\item 2\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 30\n",
       "2. 2\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 30  2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>'ctrl'</li>\n",
       "\t<li>'trt1'</li>\n",
       "\t<li>'trt2'</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 'ctrl'\n",
       "\\item 'trt1'\n",
       "\\item 'trt2'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 'ctrl'\n",
       "2. 'trt1'\n",
       "3. 'trt2'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] \"ctrl\" \"trt1\" \"trt2\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data(\"PlantGrowth\")\n",
    "dim(PlantGrowth)\n",
    "levels(PlantGrowth$group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>group</th><th scope=col>variable</th><th scope=col>n</th><th scope=col>mean</th><th scope=col>sd</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>ctrl  </td><td>weight</td><td>10    </td><td>5.032 </td><td>0.583 </td></tr>\n",
       "\t<tr><td>trt1  </td><td>weight</td><td>10    </td><td>4.661 </td><td>0.794 </td></tr>\n",
       "\t<tr><td>trt2  </td><td>weight</td><td>10    </td><td>5.526 </td><td>0.443 </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lllll}\n",
       " group & variable & n & mean & sd\\\\\n",
       "\\hline\n",
       "\t ctrl   & weight & 10     & 5.032  & 0.583 \\\\\n",
       "\t trt1   & weight & 10     & 4.661  & 0.794 \\\\\n",
       "\t trt2   & weight & 10     & 5.526  & 0.443 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| group | variable | n | mean | sd |\n",
       "|---|---|---|---|---|\n",
       "| ctrl   | weight | 10     | 5.032  | 0.583  |\n",
       "| trt1   | weight | 10     | 4.661  | 0.794  |\n",
       "| trt2   | weight | 10     | 5.526  | 0.443  |\n",
       "\n"
      ],
      "text/plain": [
       "  group variable n  mean  sd   \n",
       "1 ctrl  weight   10 5.032 0.583\n",
       "2 trt1  weight   10 4.661 0.794\n",
       "3 trt2  weight   10 5.526 0.443"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "stats_groups <- PlantGrowth %>%\n",
    "  group_by(group) %>%\n",
    "  get_summary_stats(weight, type = \"mean_sd\")\n",
    "stats_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWgAAADwCAMAAAAaeQ59AAAAMFBMVEUAAABNTU1oaGh8fHyM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\nwtLkAAAAAElFTkSuQmCC",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "options(repr.plot.width=3, repr.plot.height=2)\n",
    "ggboxplot(PlantGrowth, x = \"group\", y = \"weight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`summarise()` ungrouping output (override with `.groups` argument)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>group</th><th scope=col>mean</th><th scope=col>sd</th><th scope=col>ss</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>ctrl     </td><td>5.032    </td><td>0.5830914</td><td>3.05996  </td></tr>\n",
       "\t<tr><td>trt1     </td><td>4.661    </td><td>0.7936757</td><td>5.66929  </td></tr>\n",
       "\t<tr><td>trt2     </td><td>5.526    </td><td>0.4425733</td><td>1.76284  </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llll}\n",
       " group & mean & sd & ss\\\\\n",
       "\\hline\n",
       "\t ctrl      & 5.032     & 0.5830914 & 3.05996  \\\\\n",
       "\t trt1      & 4.661     & 0.7936757 & 5.66929  \\\\\n",
       "\t trt2      & 5.526     & 0.4425733 & 1.76284  \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| group | mean | sd | ss |\n",
       "|---|---|---|---|\n",
       "| ctrl      | 5.032     | 0.5830914 | 3.05996   |\n",
       "| trt1      | 4.661     | 0.7936757 | 5.66929   |\n",
       "| trt2      | 5.526     | 0.4425733 | 1.76284   |\n",
       "\n"
      ],
      "text/plain": [
       "  group mean  sd        ss     \n",
       "1 ctrl  5.032 0.5830914 3.05996\n",
       "2 trt1  4.661 0.7936757 5.66929\n",
       "3 trt2  5.526 0.4425733 1.76284"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# head(PlantGrowth)\n",
    "# mean(PlantGrowth$weight)\n",
    "\n",
    "PlantGrowth %>%\n",
    "  group_by(group) %>% summarise(mean=mean(weight), sd=sd(weight), ss = sum((weight-mean(weight))^2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`summarise()` ungrouping output (override with `.groups` argument)\n",
      "`summarise()` ungrouping output (override with `.groups` argument)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "10.49209"
      ],
      "text/latex": [
       "10.49209"
      ],
      "text/markdown": [
       "10.49209"
      ],
      "text/plain": [
       "[1] 10.49209"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "14.25843"
      ],
      "text/latex": [
       "14.25843"
      ],
      "text/markdown": [
       "14.25843"
      ],
      "text/plain": [
       "[1] 14.25843"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "3.76634"
      ],
      "text/latex": [
       "3.76634"
      ],
      "text/markdown": [
       "3.76634"
      ],
      "text/plain": [
       "[1] 3.76634"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "3.76634"
      ],
      "text/latex": [
       "3.76634"
      ],
      "text/markdown": [
       "3.76634"
      ],
      "text/plain": [
       "[1] 3.76634"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## total sum-squared deviation: SS_total\n",
    "ss_total_table <- PlantGrowth %>% summarise(ss = sum((weight-mean(weight))^2))\n",
    "ss_total <- ss_total_table$ss\n",
    "\n",
    "## sum of sum-squared deviations withing groups: SS_error\n",
    "ss_error_table <- PlantGrowth %>% group_by(group) %>% summarise(ss = sum((weight-mean(weight))^2))\n",
    "ss_error <- sum(ss_error_table$ss)\n",
    "\n",
    "## sum of sum-squared deviations of each sample mean from the overall mean, \n",
    "## multiplied by the number of observation in each treatment: SS_treatments\n",
    "total_mean <- mean(PlantGrowth$weight)\n",
    "n <- stats_groups$n[1]\n",
    "ss_treatments_table <- PlantGrowth %>% group_by(group) %>% summarise(ss = (mean(weight)-total_mean)^2)\n",
    "ss_treatments <- sum(ss_treatments_table$ss)*n\n",
    "\n",
    "## check SS_total = SS_error + SS_treatments\n",
    "ss_error\n",
    "ss_total\n",
    "ss_total-ss_error\n",
    "ss_treatments\n",
    "\n",
    "## DF total = DF treatment + DF error\n",
    "df_total <- length(PlantGrowth$weight)-1\n",
    "df_treatments <- length(unique(PlantGrowth$group))-1\n",
    "df_error <- length(PlantGrowth$weight) - length(unique(PlantGrowth$group))\n",
    "\n",
    "#df_total\n",
    "#df_treatments\n",
    "#df_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "4.84608786238014"
      ],
      "text/latex": [
       "4.84608786238014"
      ],
      "text/markdown": [
       "4.84608786238014"
      ],
      "text/plain": [
       "[1] 4.846088"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "F <- (ss_treatments/df_treatments)/(ss_error/df_error)\n",
    "F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Coefficient covariances computed by hccm()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>Effect</th><th scope=col>DFn</th><th scope=col>DFd</th><th scope=col>F</th><th scope=col>p</th><th scope=col>p&lt;.05</th><th scope=col>ges</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>group</td><td>2    </td><td>27   </td><td>4.846</td><td>0.016</td><td>*    </td><td>0.264</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lllllll}\n",
       " Effect & DFn & DFd & F & p & p<.05 & ges\\\\\n",
       "\\hline\n",
       "\t group & 2     & 27    & 4.846 & 0.016 & *     & 0.264\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| Effect | DFn | DFd | F | p | p<.05 | ges |\n",
       "|---|---|---|---|---|---|---|\n",
       "| group | 2     | 27    | 4.846 | 0.016 | *     | 0.264 |\n",
       "\n"
      ],
      "text/plain": [
       "  Effect DFn DFd F     p     p<.05 ges  \n",
       "1 group  2   27  4.846 0.016 *     0.264"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "res.aov <- PlantGrowth %>% anova_test(weight ~ group)\n",
    "res.aov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above ANOVA table, it can be seen that there are significant differences between groups (p = 0.016)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 4: two-way ANOVA (balanced, with interactions) <a class=\"anchor\" id=\"two_way_anova\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two-way ANOVA is a statistical procedure used to assess whether the means of a quantitative variable within several pre-defined groups differ from each other. In the **two-way ANOVA, the groups are defined by the levels of two categorical variables A and B (*factors* or *treatments*)**, differently from one-way ANOVA, where the groups were defined by the levels of just one categorical variable.\\\n",
    "As in one-way ANOVA, it's assumed that all samples are drawn from **normally distributed populations with equal variances**, $N(\\mu_{kj}, \\sigma)$ . Also it's quite important to have exactly the **same number of samples n in each analysis group (balanced ANOVA)**.\n",
    "\n",
    "The two-way ANOVA tests three null hypothesis at the same time: \n",
    "<l>\n",
    "<li>There is no difference in population means at any level of the first independent categorical variable A. Alternative hypothesis: there is a statistically significant difference in the population means related to factor A.</li>\n",
    "<li>There is no difference in population means at any level of the first independent categorical variable B. Alternative hypothesis: there is a statistically significant difference in the population means related to factor B.</li>\n",
    "<li>There is no interaction between factors A and B. The effect of one independent variable A does not depend on the effect of the other independent variable B.</li>\n",
    "</l> \n",
    "\n",
    "Steps for hypothesis testing:\n",
    "<ol>\n",
    "<li> We have $n_{groups}$ random samples each one with the <b>same number $n$ of elements (<i>balanced</i> ANOVA)</b>, where each group is identified by the levels of two categorical variables A and B ($n_{groups}=a*b$, where $a$ is the number of levels for variable A and $b$ is the number of levels for variable B). </li>\n",
    "<li> Null hypotheses:\n",
    "<ol>\n",
    "<li>$H_{0}: \\mu_{.1}=\\mu_{.2}= ...=\\mu_{.a}$</li>\n",
    "<li>$H_{0}: \\mu_{1.}=\\mu_{2.}= ...=\\mu_{b.}$</li>\n",
    "<li>There is no interaction between factors A and B. $H_{0}: \\mu_{jk}=\\mu_{total}-(\\mu_{k.}-\\mu_{total})-(\\mu_{.j}-\\mu_{total})$ where j refers to factor A and k refers to factor B. The alternative hypothesis being $H_{1}: \\mu_{jk}\\neq\\mu_{total}-(\\mu_{j.}-\\mu_{total})-(\\mu_{.k}-\\mu_{total})$ or written in another way $H_{1}: \\mu_{jk} = \\mu_{total}-(\\mu_{j.}-\\mu_{total})-(\\mu_{.k}-\\mu_{total})+(\\alpha \\beta)_{jk}$</li>\n",
    "    The term $(\\alpha \\beta)_{jk}$ is called <i>interaction term</i> and the null hypothesis assume it equal to zero. \n",
    "</ol>    \n",
    "    <li> From the observed samples data we calculate the <b>three F-test statistics</b>\n",
    "    <ol>\n",
    "<li>$F(A) =\\frac{MS_{A}}{MS_{error}} =\\frac{SS_{A}/DF_{A}}{SS_{error}/DF_{error}}$</li>\n",
    "<li>$F(B) =\\frac{MS_{B}}{MS_{error}} =\\frac{SS_{B}/DF_{B}}{SS_{error}/DF_{error}} $</li>\n",
    "<li>$F(AB) =\\frac{MS_{AB}}{MS_{error}}  =\\frac{SS_{AB}/DF_{AB}}{SS_{error}/DF_{error}}$</li>\n",
    "</ol> \n",
    "$SS_{A}= nb\\sum_{j}^{a}(\\bar{x}_{j}-\\bar{x}_{total})^2$ is based on the deviations from the grand mean of means of observations for each level of factor A, the result being multiplied by the number n of observations in each group multiplied by the number b of levels for factor B.<br>\n",
    "$SS_{B}= na\\sum_{k}^{b}(\\bar{x}_{k}-\\bar{x}_{total})^2$ is based on the deviations from the grand mean of means of observations for each level of factor B, the result being multiplied by the number n of observations in each group multiplied by the number a of levels for factor A.<br>\n",
    "$SS_{error} =\\sum_{k}^{b}\\sum_{j}^{a}\\sum_{i}^{n}(x_{ijk}- {{\\bar{x}_{jk}}})^2 $ is based on all the observation deviations from their group mean, where each group is defined by factors A and B.<br> \n",
    "$SS_{total} =\\sum_{k}^{b}\\sum_{j}^{a}\\sum_{i}^{n}(x_{ijk}- {{\\bar{x}_{total}}})^2 $ is the total sum of squares based on all the observation deviations from the grand mean. <br>\n",
    "$SS_{AB} = n\\sum_{k}^{b}\\sum_{j}^{a} (\\bar{x}_{jk}-\\bar{x}_{j}-\\bar{x}_{k}-\\bar{x}_{total})^{2}$ is the sum of squares for the interaction.<br>\n",
    "The degrees of freedom follow the following formulas: $DF_{A} =a-1 $, $DF_{B}=b-1$, $DF_{AB}=(a-1)(b-1)$, $DF_{total}=n_{total}-1$, $DF_{error}= n_{total}-ab$.<br>     \n",
    "The sum of squares follow the equation $SS_{total} = SS_{A}+SS_{B}+SS_{AB}+SS_{error}$ and the degrees of freedom $DF_{total}= DF_{A}+DF_{B}+DF_{AB}+DF_{error}$.<br>\n",
    "\n",
    "For each one of the null hypothesis, the assumption implies that the corresponding statistic <b>F will follow a F-distribution</b>.\n",
    "</li>\n",
    "<li> <b>Comparing the observed value of the statistics with the corresponding distribution</b>, we can find the likelihood that a value as extreme as or more than the observed one is found by chance. This is the <b>p-value</b>.</li>\n",
    "<li> If <b>p < 0.05</b> (or below a threshold we decide), we <b>reject the null hypothesis</b> and speak of a statistically significant difference. </li>\n",
    "</ol>  \n",
    "\n",
    "<br />\n",
    "The two-way ANOVA stands on these main assumptions:\n",
    "\n",
    "- independence of observations.\n",
    "- **normality of distributions** in each group.\n",
    "- equality (or \"homogeneity\") of variances, called **homoscedasticity** — the variance of data in groups should be the same.\n",
    "- same number of samples in each analysis group (*balanced* ANOVA).\n",
    "- no significant outliers in any cell of the design.\n",
    "\n",
    "https://www.itl.nist.gov/div898/handbook/prc/section4/prc437.htm \\\n",
    "https://www.scribbr.com/statistics/two-way-anova/\n",
    "\n",
    "Check here for interaction term and explicit null hypothesis for it:\\\n",
    "https://learningstatisticswithr.com/book/anova2.html\n",
    "\n",
    "Check here for balanced three-way ANOVA and unbalanced ANOVA:\\\n",
    "http://users.stat.umn.edu/~helwig/notes/aov2-Notes.pdf\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Numerical Example**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the built-in R data set named ToothGrowth. It contains data from a study evaluating the effect of vitamin C on tooth growth in Guinea pigs. The experiment has been performed on 60 pigs, where each animal received one of three dose levels of vitamin C (0.5, 1, and 2 mg/day) by one of two delivery methods, orange juice (OJ) or ascorbic acid (VC).\n",
    "\n",
    "Here we assume that we have done all the necessary tests for checking normality and homoscedasticity conditions.\n",
    "\n",
    "http://www.sthda.com/english/wiki/two-way-anova-test-in-r\n",
    "\n",
    "Check for example of unbalanced two-way ANOVA:\\\n",
    "https://www.datanovia.com/en/lessons/anova-in-r/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(tidyverse)\n",
    "library(dplyr)\n",
    "library(ggpubr)\n",
    "library(rstatix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(123)\n",
    "my_data <- ToothGrowth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ATT! It's important to have categorical variables for groups definitions!!!\n",
    "my_data$supp <- as.factor(my_data$supp)\n",
    "my_data$dose <- as.factor(my_data$dose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check a random sample from the dataset\n",
    "# my_data %>% sample_n_by(supp, dose, size = 1)\n",
    "\n",
    "# mean and sd per group\n",
    "sum_stats <- my_data %>%\n",
    "  group_by(supp, dose) %>%\n",
    "  get_summary_stats(len, type = \"mean_sd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>supp</th><th scope=col>dose</th><th scope=col>variable</th><th scope=col>n</th><th scope=col>mean</th><th scope=col>sd</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>OJ   </td><td>0.5  </td><td>len  </td><td>10   </td><td>13.23</td><td>4.460</td></tr>\n",
       "\t<tr><td>OJ   </td><td>1    </td><td>len  </td><td>10   </td><td>22.70</td><td>3.911</td></tr>\n",
       "\t<tr><td>OJ   </td><td>2    </td><td>len  </td><td>10   </td><td>26.06</td><td>2.655</td></tr>\n",
       "\t<tr><td>VC   </td><td>0.5  </td><td>len  </td><td>10   </td><td> 7.98</td><td>2.747</td></tr>\n",
       "\t<tr><td>VC   </td><td>1    </td><td>len  </td><td>10   </td><td>16.77</td><td>2.515</td></tr>\n",
       "\t<tr><td>VC   </td><td>2    </td><td>len  </td><td>10   </td><td>26.14</td><td>4.798</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llllll}\n",
       " supp & dose & variable & n & mean & sd\\\\\n",
       "\\hline\n",
       "\t OJ    & 0.5   & len   & 10    & 13.23 & 4.460\\\\\n",
       "\t OJ    & 1     & len   & 10    & 22.70 & 3.911\\\\\n",
       "\t OJ    & 2     & len   & 10    & 26.06 & 2.655\\\\\n",
       "\t VC    & 0.5   & len   & 10    &  7.98 & 2.747\\\\\n",
       "\t VC    & 1     & len   & 10    & 16.77 & 2.515\\\\\n",
       "\t VC    & 2     & len   & 10    & 26.14 & 4.798\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| supp | dose | variable | n | mean | sd |\n",
       "|---|---|---|---|---|---|\n",
       "| OJ    | 0.5   | len   | 10    | 13.23 | 4.460 |\n",
       "| OJ    | 1     | len   | 10    | 22.70 | 3.911 |\n",
       "| OJ    | 2     | len   | 10    | 26.06 | 2.655 |\n",
       "| VC    | 0.5   | len   | 10    |  7.98 | 2.747 |\n",
       "| VC    | 1     | len   | 10    | 16.77 | 2.515 |\n",
       "| VC    | 2     | len   | 10    | 26.14 | 4.798 |\n",
       "\n"
      ],
      "text/plain": [
       "  supp dose variable n  mean  sd   \n",
       "1 OJ   0.5  len      10 13.23 4.460\n",
       "2 OJ   1    len      10 22.70 3.911\n",
       "3 OJ   2    len      10 26.06 2.655\n",
       "4 VC   0.5  len      10  7.98 2.747\n",
       "5 VC   1    len      10 16.77 2.515\n",
       "6 VC   2    len      10 26.14 4.798"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sum_stats "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeAAAAGkCAMAAADewwbdAAAANlBMVEUAAAAAc8JNTU1oaGh8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\nwOIBLN5/l/16VKKuUEgAAAAASUVORK5CYII=",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "options(repr.plot.width=4, repr.plot.height=3.5)\n",
    "bxp <- ggboxplot(\n",
    "  my_data, x = \"dose\", y = \"len\",\n",
    "  color = \"supp\", palette = \"jco\"\n",
    "  )\n",
    "bxp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "TRUE"
      ],
      "text/latex": [
       "TRUE"
      ],
      "text/markdown": [
       "TRUE"
      ],
      "text/plain": [
       "[1] TRUE"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## number of observations and groups\n",
    "n <- sum_stats$n[1] # number of observation per group: same for every group in balanced anova\n",
    "n_total <- length(my_data$len)\n",
    "n_groups <- nrow(sum_stats)\n",
    "n_dose <- length(unique(sum_stats$dose))\n",
    "n_supp <- length(unique(sum_stats$supp))\n",
    "\n",
    "## degrees of freedom\n",
    "df_total <- n_total-1\n",
    "df_dose <- n_dose-1\n",
    "df_supp <- n_supp-1\n",
    "df_error <- n_total-n_dose*n_supp\n",
    "df_dose_supp <- (n_dose-1)*(n_supp-1)\n",
    "\n",
    "## check degrees of freedom equivalence\n",
    "df_dose+df_supp+df_error+df_dose_supp == df_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`summarise()` ungrouping output (override with `.groups` argument)\n",
      "`summarise()` ungrouping output (override with `.groups` argument)\n",
      "`summarise()` regrouping output by 'dose' (override with `.groups` argument)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "TRUE"
      ],
      "text/latex": [
       "TRUE"
      ],
      "text/markdown": [
       "TRUE"
      ],
      "text/plain": [
       "[1] TRUE"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "total_mean <- mean(my_data$len)\n",
    "\n",
    "## sum-squared deviations\n",
    "ss_total_table <- my_data %>% summarise(ss = sum((len-total_mean)^2))\n",
    "ss_total <- ss_total_table$ss\n",
    "\n",
    "ss_dose_table <- my_data %>% group_by(dose) %>% summarise(ss = (mean(len)-total_mean)^2)\n",
    "ss_dose <- sum(ss_dose_table$ss)*n*n_supp\n",
    "\n",
    "ss_supp_table <- my_data %>% group_by(supp) %>% summarise(ss = (mean(len)-total_mean)^2)\n",
    "ss_supp <- sum(ss_supp_table$ss)*n*n_dose\n",
    "\n",
    "ss_error_table <- my_data %>% group_by(dose, supp) %>% summarise(ss = sum((len-mean(len))^2))\n",
    "ss_error <- sum(ss_error_table$ss)\n",
    "\n",
    "ss_dose_supp_table <- my_data %>%\n",
    "  group_by(dose) %>% \n",
    "  mutate(mean_dose = mean(len)) %>% \n",
    "  ungroup() %>% \n",
    "  group_by(supp) %>% \n",
    "  mutate(mean_supp = mean(len)) %>% \n",
    "  ungroup()  %>%\n",
    "  group_by(dose, supp) %>% \n",
    "  mutate(mean_dose_supp = mean(len)) %>% \n",
    "  ungroup()\n",
    "\n",
    "ss_dose_supp <- sum((ss_dose_supp_table$mean_dose_supp-ss_dose_supp_table$mean_dose-ss_dose_supp_table$mean_supp+total_mean)^2)\n",
    "\n",
    "## check equivalence\n",
    "round(ss_dose+ss_supp+ss_error+ss_dose_supp,3) == round(ss_total,3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "15.5719794524972"
      ],
      "text/latex": [
       "15.5719794524972"
      ],
      "text/markdown": [
       "15.5719794524972"
      ],
      "text/plain": [
       "[1] 15.57198"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "91.9999648928671"
      ],
      "text/latex": [
       "91.9999648928671"
      ],
      "text/markdown": [
       "91.9999648928671"
      ],
      "text/plain": [
       "[1] 91.99996"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "4.10699109402252"
      ],
      "text/latex": [
       "4.10699109402252"
      ],
      "text/markdown": [
       "4.10699109402252"
      ],
      "text/plain": [
       "[1] 4.106991"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## F values\n",
    "F_dose <- (ss_dose/df_dose)/(ss_error/df_error)\n",
    "F_supp <- (ss_supp/df_supp)/(ss_error/df_error)\n",
    "F_dose_supp <- (ss_dose_supp/df_dose_supp)/(ss_error/df_error)\n",
    "F_supp\n",
    "F_dose\n",
    "F_dose_supp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "            Df Sum Sq Mean Sq F value   Pr(>F)    \n",
       "supp         1  205.4   205.4  15.572 0.000231 ***\n",
       "dose         2 2426.4  1213.2  92.000  < 2e-16 ***\n",
       "supp:dose    2  108.3    54.2   4.107 0.021860 *  \n",
       "Residuals   54  712.1    13.2                     \n",
       "---\n",
       "Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Two-way ANOVA with interaction effect. NOTE: aov is designed for BALANCED DESIGNS!!!\n",
    "res.aov <- aov(len ~ supp + dose + supp:dose, data = my_data)\n",
    "# res.aov <- aov(len ~ supp * dose , data = my_data) ## equivalent call\n",
    "summary(res.aov)\n",
    "\n",
    "## Note that, in the situation where the interaction is \n",
    "## not significant you should use the additive model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can conclude:\n",
    "<l>\n",
    "<li>There is a statistically significant difference in the population means related to the delivery method (supp). This indicates that the levels of supp are associated with significant different tooth length.</li>\n",
    "<li>There is a statistically significant difference in the population means related the dose level (dose). This indicates that the levels of dose are associated with significant different tooth length.</li>\n",
    "<li>There is statistically significant interaction between the delivery method and the dose level.  This indicates that the relationships between dose and tooth length depends on the supp method.</li>\n",
    "</l> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests of frequencies of categorical data <a class=\"anchor\" id=\"tests_categorical_data\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **analysis of categorical data is analysis of frequencies**. Here we consider the most common hypothesis testing techniques for **comparing frequencies across groups**, i.e. how the observations are distributed across groups.\n",
    "\n",
    "When two or more groups are compared, the data are often shown in the form of a **frequency table, also called contingency table**. The contingency table shows in every cell the observed frequency for a group, i.e. the total number of observations belonging to that group. Each group is defined by the levels of a certain number of categories (*factors*), that define the dimensionality of the contingency table. The contingency table also contains totals for each factor.\n",
    "\n",
    "As an example, if we consider groups defined by **two factors each one with two levels**, the **contingency table has the following 2x2 structure**:\n",
    "\n",
    "<table>\n",
    "  <caption>2x2 contingency table:</caption>\n",
    "  <tr>\n",
    "    <td></td>\n",
    "    <th scope=\"col\">B = 0</th>\n",
    "    <th scope=\"col\">B = 1</th>\n",
    "    <th scope=\"col\">Total</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th scope=\"row\">A = 0</th>\n",
    "    <td>a</td>\n",
    "    <td>b</td>\n",
    "    <td>a+b</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th scope=\"row\">A = 1</th>\n",
    "    <td>c</td>\n",
    "    <td>d</td>\n",
    "    <td>c+d</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <th scope=\"row\">Total</th>\n",
    "    <td>a+c</td>\n",
    "    <td>b+d</td>\n",
    "    <td>a+b+c+d</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "where A and B are the two factors, each one having levels 0 and 1.\n",
    "\n",
    "Here we consider tests for **one-way contingency tables**, i.e. cases in which we have just **one categorical variable** (groups defined by the levels of one categorical variable) and tests for **two-way contingency tables**, i.e. cases in which we have **two categorical variables** (groups defined by the levels of two categorical variables). For tests analysing **three-way or more contingency tables** check the *cran.r-project* document.\n",
    "\n",
    "Many of the tests considered here, are based on **analyzing the deviation of the observed frequencies from their expected value**.\n",
    "\n",
    "\n",
    "Book: An Introduction to Statistics with Python \\\n",
    "wiki - Chi-squared test \\\n",
    "wiki - Pearson's chi-squared test \\\n",
    "https://cran.r-project.org/web/packages/vcdExtra/vignettes/vcd-tutorial.pdf \\\n",
    "https://sphweb.bumc.bu.edu/otlt/mph-modules/bs/bs704_hypothesistesting-chisquare/bs704_hypothesistesting-chisquare_print.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 1: One-way Pearson Chi-Square test - goodness of fit <a class=\"anchor\" id=\"one_way_chi2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The one-way Chi-Square test is a statistical procedure used to compare **frequencies of observations across groups** in the case groups are defined by just **one categorical variable or factor**. The **goal here is to assess if a sample of observations could have been take from a population with a specific probability distribution $p_{i}$ across groups**.\n",
    "\n",
    "The null hypothesis assumes that the population has a specific probability distribution, with probability $p_{i}$ that an observation falls into the $i_{th}$ group, i.e. $H_{0}: p_{1}=p_{01}, p_{2}=p_{02}, \\dots, p_{k}=p_{0k}$, where $k$ is the number of **groups, defined by the levels of one categorical variable**. The alternative hypothesis corresponds in a statistically significant difference of the population distribution across groups in comparison to the one assumed by the null hypothesis.\n",
    "\n",
    "<ol>\n",
    "<li> We have $n$ observations in a random sample from a population, classified into k mutually exclusive classes with respective observed numbers $x_{i}$ (for i = 1,2,…,k).</li>\n",
    "<li> The null hypothesis $H_{0}: p_{1}=p_{01}, p_{2}=p_{02}, \\dots, p_{k}=p_{0k}$, gives the probability $p_{i}$ that an observation falls into the ith class. Under this assumption, the expected frequencies are determined by allocating the observations to the groups according to the distribution specified in $H_{0}$. This is done by multiplying the observed sample size $n$ by the probabilities specified in the null hypothesis ($p_{01}, p_{02}, \\dots, p_{0k}$). </li>\n",
    "<li> From the observed sample data we calculate the test-statistic <br> \n",
    "    $V = \\sum \\frac{(O-E)^2}{E}$ <br> \n",
    "    where O = <i>observed frequency</i> and E = <i>expected frequency</i>\n",
    "    in each of the response categories (groups). <br>\n",
    "    If we assume the null hypothesis, the <b>$V$ test statistic can be approximated (under the assumptions stated below) by a $\\chi^2$ distribution with k − 1 degrees of freedom </b>.   \n",
    "</li>\n",
    "<li> <b>Comparing the observed value of the statistics with the corresponding distribution</b>, we can find the likelihood that a value as extreme as or more than the observed one is found by chance. This is the <b>p-value</b>.</li>\n",
    "<li> If <b>p < 0.05</b> (or below a threshold we decide), we <b>reject the null hypothesis</b> and speak of a statistically significant difference. </li>\n",
    "</ol>  \n",
    "\n",
    "\n",
    "For assumptions check Two-way Pearson Chi-Square test - test of independence below. (Pearson's chi-squared test - wiki)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Numerical Example**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A University conducted a survey of its recent graduates. In response to a question on regular exercise, 60% of all graduates reported getting no regular exercise, 25% reported exercising sporadically and 15% reported exercising regularly as undergraduates. The next year the University launched a health promotion campaign on campus in an attempt to increase health behaviors among undergraduates. To evaluate the impact of the program, the University again surveyed graduates and asked the same questions. The survey was completed by 470 graduates and the following data were collected on the exercise question:\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <td></td>\n",
    "    <th scope=\"col\">No exercise</th>\n",
    "    <th scope=\"col\">Sporadic Exarcise</th>\n",
    "    <th scope=\"col\">Regular Exercise</th>\n",
    "    <th scope=\"col\">Total</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th scope=\"row\">Number of students</th>\n",
    "    <td>255</td>\n",
    "    <td>125</td>\n",
    "    <td>90</td>\n",
    "    <td>470</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "Based on the data, we want to know: is there evidence of a shift in the distribution of responses to the exercise question following the implementation of the health promotion campaign on campus?\n",
    "\n",
    "$H0: p_{1}=0.60, p_{2}=0.25, p_{3}=0.15$\n",
    "\n",
    "We assume that all assumption for using $\\chi^2$ test have been checked.\n",
    "\n",
    " https://sphweb.bumc.bu.edu/otlt/mph-modules/bs/bs704_hypothesistesting-chisquare/bs704_hypothesistesting-chisquare_print.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate Chi2\n",
    "O <- c(255, 125, 90)\n",
    "n <- sum(O)\n",
    "\n",
    "p <- c(0.60, 0.25, 0.15)\n",
    "E <- p*n\n",
    "\n",
    "chi2 <- sum((O-E)^2/E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "8.45744680851064"
      ],
      "text/latex": [
       "8.45744680851064"
      ],
      "text/markdown": [
       "8.45744680851064"
      ],
      "text/plain": [
       "[1] 8.457447"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "\tChi-squared test for given probabilities\n",
       "\n",
       "data:  O\n",
       "X-squared = 8.4574, df = 2, p-value = 0.01457\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chisq.test(O, p=p, correct=FALSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we reject $H_{0}$ and concluded that the distribution of responses to the exercise question following the implementation of the health promotion campaign was not the same as the distribution prior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 2: Two-way Pearson Chi-Square test - test of independence <a class=\"anchor\" id=\"two_way_chi2_independence\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now consider the situation where groups are defined by **two categorical variables or factors A and B**. Here the Chi-Square test of independence is used to **determine if there is a significant relationship between two nominal (categorical) variables**. The frequency of each level for one nominal variable is compared across the levels of the second nominal variable. \n",
    "\n",
    "The null hypothesis states that knowing the level of variable A does not help you predict the level of variable B. That is, the variables are independent. The alternative hypothesis is that knowing the level of variable A can help you predict the level of variable B.\n",
    "\n",
    "<ol>\n",
    "<li> We have $n$ observations in a random sample from a population, classified into $n_{groups}$, where each group is identified by the levels of two categorical variables A and B ($n_{groups}=a*b$, where $a$ is the number of levels for variable A and $b$ is the number of levels for variable B), with respective observed numbers $x_{jk}$. </li>\n",
    "<li> Null hypothesis: variable A and B are independent. <br>\n",
    " $P(A\\&B)=P(A)P(B)$ <br>\n",
    "So the the null hypothesis can be written in a tabular form \n",
    "<table>\n",
    "  <caption>Null hypothesis:</caption>\n",
    "  <tr>\n",
    "    <td></td>\n",
    "    <th scope=\"col\">B = 1 </th>\n",
    "    <th scope=\"col\">B = 2 </th>\n",
    "    <th scope=\"col\"> ... </th>\n",
    "    <th scope=\"col\">B = b </th> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th scope=\"row\">A = 1</th>\n",
    "    <td>$p_{11}$=$p_{1.}$$p_{.1}$</td>\n",
    "    <td>$p_{12}$=$p_{1.}$$p_{.2}$</td>\n",
    "    <td>...</td>\n",
    "    <td>$p_{1b}$=$p_{1.}$$p_{.b}$</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th scope=\"row\">A = 2</th>\n",
    "    <td>$p_{21}$=$p_{2.}$$p_{.1}$</td>\n",
    "    <td>$p_{22}$=$p_{2.}$$p_{.2}$</td>\n",
    "    <td>...</td>\n",
    "    <td>$p_{2b}$=$p_{2.}$$p_{.b}$</td>\n",
    "  </tr>\n",
    "     <tr>\n",
    "    <th scope=\"row\"> ... </th>\n",
    "    <td>...</td>\n",
    "    <td>...</td>\n",
    "    <td>...</td>\n",
    "    <td>...</td>\n",
    "  </tr>\n",
    "      <tr>\n",
    "    <th scope=\"row\">A = a</th>\n",
    "    <td>$p_{a1}$=$p_{a.}$$p_{.1}$</td>\n",
    "    <td>$p_{a2}$=$p_{a.}$$p_{.2}$</td>\n",
    "    <td>...</td>\n",
    "    <td>$p_{ab}$=$p_{a.}$$p_{.b}$</td>\n",
    "  </tr>\n",
    "</table>    \n",
    "        \n",
    "where 𝑝jk gives the probability that an observation falls into the jkth group (level j of variable A and level k of variable B).\n",
    "Given the observed contingency table, of frequencies in each group \n",
    "<table>\n",
    "  <caption>Null hypothesis:</caption>\n",
    "  <tr>\n",
    "    <td></td>\n",
    "    <th scope=\"col\">B = 1 </th>\n",
    "    <th scope=\"col\">B = 2 </th>\n",
    "    <th scope=\"col\"> ... </th>\n",
    "    <th scope=\"col\">Total</th> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th scope=\"row\">A = 1</th>\n",
    "    <td>$x_{11}$</td>\n",
    "    <td>$x_{12}$</td>\n",
    "    <td>...</td>\n",
    "    <td>$\\sum$ $x_{1k}$</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th scope=\"row\">A = 2</th>\n",
    "    <td>$x_{21}$</td>\n",
    "    <td>$x_{22}$</td>\n",
    "    <td>...</td>\n",
    "    <td>$\\sum$ $x_{2k}$</td>\n",
    "  </tr>\n",
    "     <tr>\n",
    "    <th scope=\"row\"> ... </th>\n",
    "    <td>...</td>\n",
    "    <td>...</td>\n",
    "    <td>...</td>\n",
    "    <td>...</td>\n",
    "  </tr>\n",
    "      <tr>\n",
    "    <th scope=\"row\">Total</th>\n",
    "    <td>$\\sum$ $x_{j1}$</td>\n",
    "    <td>$\\sum$ $x_{j2}$</td>\n",
    "    <td>...</td>\n",
    "    <td>$n =$$\\sum$ $x_{jk}$</td>\n",
    "  </tr>\n",
    "</table>     \n",
    "    \n",
    "we can estimate such probabilities. Our best estimate is given by: <br>\n",
    "    \n",
    "$p_{jk} = p_{j.}p_{.k} \\approx \\frac{\\sum^a_{i} x_{ji}}{n}\\frac{\\sum^b_{i} x_{ik}}{n}$ \n",
    "    \n",
    "The expected frequency is found by multiplying $E_{jk} =n p_{jk}$.\n",
    "<li> From the observed sample data we calculate the test-statistic: <br> \n",
    "    $V = \\sum \\frac{(O-E)^2}{E}$ <br> \n",
    "    where O = <i>observed frequency</i> and E = <i>expected frequency</i>\n",
    "    in each of the response categories (groups). <br>\n",
    "    If we assume the null hypothesis, the <b> $V$ test statistic  can be approximated (under the assumptions stated below) by a $\\chi^2$ distribution with $df = (a-1)(b-1)$ degrees of freedom </b>.   \n",
    "</li>\n",
    "<li> <b>Comparing the observed value of the statistics with the corresponding distribution</b>, we can find the likelihood that a value as extreme as or more than the observed one is found by chance. This is the <b>p-value</b>.</li>\n",
    "<li> If <b>p < 0.05</b> (or below a threshold we decide), we <b>reject the null hypothesis</b> and speak of a statistically significant difference. </li>\n",
    "</ol>  \n",
    "\n",
    "Assumptions:\n",
    "- **Simple random sample**: The sample data is a random sampling from a fixed distribution or population where every collection of members of the population of the given sample size has an equal probability of selection.\n",
    "- **Sample size (whole table)**: A sample with a sufficiently large size is assumed. If a chi squared test is conducted on a sample with a smaller size, then the chi squared test will yield an inaccurate inference.\n",
    "- **Expected cell count**: Adequate expected cell counts. Some require 5 or more, and others require 10 or more. A common rule is 5 or more in all cells of a 2-by-2 table, and 5 or more in 80% of cells in larger tables, but no cells with zero expected count.  When this assumption is not met, Yates's correction is applied.\n",
    "- **Independence**: The observations are always assumed to be independent of each other. This means chi-squared cannot be used to test correlated data (like matched pairs or panel data). In those cases, McNemar's test may be more appropriate.\n",
    "\n",
    "\n",
    "https://sphweb.bumc.bu.edu/otlt/mph-modules/bs/bs704_hypothesistesting-chisquare/bs704_hypothesistesting-chisquare_print.html \\\n",
    "https://www.statisticssolutions.com/non-parametric-analysis-chi-square/ \\\n",
    "https://www.khanacademy.org/math/ap-statistics/chi-square-tests/chi-square-tests-two-way-tables/v/chi-square-test-association-independence \\\n",
    "Chi-squared test - wiki \\\n",
    "Pearson's chi-squared test - wiki"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Numerical Example**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have data from a survey of university graduates which assessed, among other things, how frequently they exercised and their living arrangements. The survey was completed by 470 graduates.\n",
    "We wish to assess whether there is a relationship between exercise on campus and students' living arrangements.\n",
    "Based on the data, is there a relationship between exercise and student's living arrangement? Do you think where a person lives affect their exercise status?\n",
    "\n",
    "<table>\n",
    "  <caption>Contingency table:</caption>\n",
    "  <tr>\n",
    "    <td></td>\n",
    "    <th scope=\"col\">No Exercise</th>\n",
    "    <th scope=\"col\">Sporadic Exercise</th>\n",
    "    <th scope=\"col\">Regular Exercise</th>  \n",
    "    <th scope=\"col\">Total</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th scope=\"row\">Dormitory</th>\n",
    "    <td>32</td>\n",
    "    <td>30</td>\n",
    "        <td>28</td>\n",
    "    <td>90</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th scope=\"row\">On-Campus Apartment</th>\n",
    "    <td>74</td>\n",
    "    <td>64</td>\n",
    "     <td>42</td>\n",
    "    <td>180</td>\n",
    "  </tr>\n",
    "      <tr>\n",
    "    <th scope=\"row\">Off-Campus Apartment</th>\n",
    "    <td>110</td>\n",
    "    <td>25</td>\n",
    "     <td>15</td>\n",
    "    <td>150</td>\n",
    "  </tr>\n",
    "      <tr>\n",
    "    <th scope=\"row\">At Home</th>\n",
    "    <td>39</td>\n",
    "    <td>6</td>\n",
    "     <td>5</td>\n",
    "    <td>50</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <th scope=\"row\">Total</th>\n",
    "    <td>255</td>\n",
    "    <td>125</td>\n",
    "    <td>90</td>\n",
    "    <td>470</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "https://sphweb.bumc.bu.edu/otlt/mph-modules/bs/bs704_hypothesistesting-chisquare/bs704_hypothesistesting-chisquare_print.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate Chi2\n",
    "matrix <- cbind(c(32,74,110,39),c(30,64,25,6),c(28,42,15,5)) \n",
    "colnames(matrix) <- c(\"no_exercise\",\"sporadic_exercise\",\"regular_exercise\")\n",
    "rownames(matrix) <- c(\"dormitory\",\"on_campus_apartment\",\"off_campus_apartment\", \"home\")\n",
    "O <- data.frame(matrix)\n",
    "\n",
    "n <- sum(O)\n",
    "exercise_p <- colSums(O)/n\n",
    "living_p <- rowSums(O)/n\n",
    "matrix_expected <- cbind(living_p*exercise_p[\"no_exercise\"],living_p*exercise_p[\"sporadic_exercise\"],\n",
    "                         living_p*exercise_p[\"regular_exercise\"])\n",
    "colnames(matrix_expected) <- c(\"no_exercise\",\"sporadic_exercise\",\"regular_exercise\")\n",
    "E <- matrix_expected*n\n",
    "\n",
    "Chi2 <- sum((O-E)^2/E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "60.4394469135803"
      ],
      "text/latex": [
       "60.4394469135803"
      ],
      "text/markdown": [
       "60.4394469135803"
      ],
      "text/plain": [
       "[1] 60.43945"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "\tPearson's Chi-squared test\n",
       "\n",
       "data:  O\n",
       "X-squared = 60.439, df = 6, p-value = 3.664e-11\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chisq.test(O, correct=FALSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we reject  𝐻0  and concluded that there is significant relationship between exercise on campus and students' living arrangements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 2: Two-way Pearson Chi-Square test - test for homogeneity <a class=\"anchor\" id=\"two_way_chi2_homogeneity\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Chi-square test for homogeneity the Chi-square test is not used, as for the two-way Chi-Suare test of independence, to determine if there is a relationship between two categorical variables that describe a random sample taken from a population.\n",
    "Here, in the **Chi-Square test of homogeneity, we are going to look at two different groups and see if the distributions of those groups for a certain categorical variable are similar or not**.\n",
    "\n",
    "**Here we sample from two different populations.**\n",
    "\n",
    "Our null hypothesis is that there is **no difference in the distribution of the two populations in terms of the categorical variable we are considering**. The alternative hypothesis is that there is a difference.\n",
    "\n",
    "<ol>\n",
    "<li> We have $n_{1}$ observations of one categorical variable A in a random sample from a population 1 (group 1) and $n_{2}$ observations of the same categorical variable A in a random sample from a population 2 (group 2), where $x_{jk}$ are the respective observed frequencies. In this case k takes only two possible values k=0,1. </li>\n",
    "<li> Null hypothesis: the distribution of variable A is the same for the two populations 1 and 2. <br>\n",
    "Under this assumption we can calculate the expected values of the frequencies for each level of category A in the two groups.   \n",
    "    \n",
    "Given the observed contingency table of frequencies in each group: \n",
    "<table>\n",
    "  <caption>Null hypothesis:</caption>\n",
    "  <tr>\n",
    "    <td></td>\n",
    "    <th scope=\"col\"> group = 1 </th>\n",
    "    <th scope=\"col\"> group = 2 </th>\n",
    "    <th scope=\"col\">Total</th> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th scope=\"row\">A = 1</th>\n",
    "    <td>$x_{11}$</td>\n",
    "    <td>$x_{12}$</td>\n",
    "    <td>$\\sum$ $x_{1k}$</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th scope=\"row\">A = 2</th>\n",
    "    <td>$x_{21}$</td>\n",
    "    <td>$x_{22}$</td>\n",
    "    <td>$\\sum$ $x_{2k}$</td>\n",
    "  </tr>\n",
    "     <tr>\n",
    "    <th scope=\"row\"> ... </th>\n",
    "    <td>...</td>\n",
    "    <td>...</td>\n",
    "    <td>...</td>\n",
    "  </tr>\n",
    "      <tr>\n",
    "    <th scope=\"row\">Total</th>\n",
    "    <td>$\\sum$ $x_{j1}$</td>\n",
    "    <td>$\\sum$ $x_{j2}$</td>\n",
    "    <td>$n =$$\\sum$ $x_{jk}$</td>\n",
    "  </tr>\n",
    "</table> \n",
    "    \n",
    "our best guess for the probability of obtaining A = j is independent of the group and equal to:\n",
    "\n",
    "$p_{j.} = \\frac{\\sum^1_{0} x_{jk}}{n} =\\frac{x_{j1}+x_{j2}}{n}$\n",
    " \n",
    "So the expected frequency $E_{jk}$ is found by simply multiplying this probability by the total number of observations in group k:\n",
    "    \n",
    "$E_{jk} = p_{j.} \\sum^{a}_{i} x_{ik} = (\\sum^1_{0} x_{jk} \\sum^{a}_{i} x_{ik})/n$\n",
    "    \n",
    "<b>Even if the Chi-Square test of homogeneity is trying to answer a different question if compared to the Chi-Squared test of independence, we observe that the $E_{jk}$ that we obtain is exactly the same, so the two problems can be described in the same way. </b>\n",
    "   \n",
    "<li>The following steps are exactly the same as the ones presented in the Chi-Square for independence.  </li>\n",
    "</ol>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Numerical Example**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider left-handed people vs right handed people. We are wondering: do they have the same preference for subject domains? \\\n",
    "Our null hypothesis is that the is no difference in the distribution of right-handed and left-handed people in terms of their preferences for subject domains. The alternative hypothesis is that there is a difference.\n",
    "\n",
    "From the right-handed population we exctract a random sample of $n_{r}=60$ right-handed people and from the left-handed population we extract a random sample of $n_{l}=40$ left-handed people.\n",
    "\n",
    "Here is the data collected about the favorite subject of the people from the two samples:\n",
    "\n",
    "<table>\n",
    "  <caption>Contingency table:</caption>\n",
    "  <tr>\n",
    "    <td></td>\n",
    "    <th scope=\"col\">Right</th>\n",
    "    <th scope=\"col\">Left</th>\n",
    "    <th scope=\"col\">Total</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th scope=\"row\">STEM</th>\n",
    "    <td>30</td>\n",
    "    <td>10</td>\n",
    "    <td>40</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th scope=\"row\">Humanities</th>\n",
    "    <td>15</td>\n",
    "    <td>25</td>\n",
    "    <td>40</td>\n",
    "  </tr>\n",
    "      <tr>\n",
    "    <th scope=\"row\">Equal</th>\n",
    "    <td>15</td>\n",
    "    <td>5</td>\n",
    "     <td>20</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <th scope=\"row\">Total</th>\n",
    "    <td>60</td>\n",
    "    <td>40</td>\n",
    "    <td>100</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "https://www.khanacademy.org/math/ap-statistics/chi-square-tests/chi-square-tests-two-way-tables/v/chi-square-test-homogeneity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate Chi2\n",
    "matrix <- cbind(c(30, 15, 15),c(10, 25, 5)) \n",
    "colnames(matrix) <- c(\"right\", \"left\")\n",
    "rownames(matrix) <- c(\"STEM\", \"humanities\", \"equal\")\n",
    "O <- data.frame(matrix)\n",
    "\n",
    "n_right <- colSums(O)[\"right\"]\n",
    "n_left <- colSums(O)[\"left\"]\n",
    "\n",
    "subjects_right_p <- rowSums(O)/100*n_right\n",
    "subjects_left_p <- rowSums(O)/100*n_left\n",
    "\n",
    "matrix_expected <- cbind(subjects_right_p, subjects_left_p)\n",
    "colnames(matrix_expected) <- c(\"right\", \"left\")\n",
    "\n",
    "E <- data.frame(matrix_expected)\n",
    "\n",
    "Chi2 <- sum((O-E)^2/E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "14.0625"
      ],
      "text/latex": [
       "14.0625"
      ],
      "text/markdown": [
       "14.0625"
      ],
      "text/plain": [
       "[1] 14.0625"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "\tPearson's Chi-squared test\n",
       "\n",
       "data:  O\n",
       "X-squared = 14.062, df = 2, p-value = 0.0008838\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chisq.test(O, correct=FALSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we reject  𝐻0  and concluded that there is a difference in the distribution of right-handed and left-handed people in terms of their preferences for subject domains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate power of the test\n",
    "c <- min(c(dim(O)[1],dim(O)[2]))\n",
    "sig_level <- 0.05\n",
    "n <- sum(O)\n",
    "df <- (dim(O)[1]-1)*(dim(O)[2]-1)\n",
    "\n",
    "## effect size (Cramer's)\n",
    "w <- sqrt(Chi2/(n*(c-1)))\n",
    "\n",
    "## Check here for more details about Cramer's effect size:\n",
    "## https://www.datascienceblog.net/post/statistical_test/effect_size/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.375"
      ],
      "text/latex": [
       "0.375"
      ],
      "text/markdown": [
       "0.375"
      ],
      "text/plain": [
       "[1] 0.375"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "0.375"
      ],
      "text/latex": [
       "0.375"
      ],
      "text/markdown": [
       "0.375"
      ],
      "text/plain": [
       "[1] 0.375"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "library(lsr)\n",
    "w1 <- cramersV(O, correct=FALSE)\n",
    "w1\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "     Chi squared power calculation \n",
       "\n",
       "              w = 0.375\n",
       "              n = 100\n",
       "             df = 2\n",
       "      sig.level = 0.05\n",
       "          power = 0.9291329\n",
       "\n",
       "NOTE: n is the number of observations\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "library(DescTools)\n",
    "power.chisq.test(w = w, df = df, sig.level = sig_level, power = NULL, n = n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a 93% chance of correctly rejecting the null hypothesis, assuming that the magnitude of the effect observed is true (difference between the two groups)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "     Chi squared power calculation \n",
       "\n",
       "              w = 0.375\n",
       "              n = 68.51334\n",
       "             df = 2\n",
       "      sig.level = 0.05\n",
       "          power = 0.8\n",
       "\n",
       "NOTE: n is the number of observations\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "power.chisq.test(w = w, df = df, sig.level = sig_level, power = 0.8, n = NULL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would have needed only 68 total observations to correctly reject the null hypothesis with a 80% probability (assuming the effect size we observe as true)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cramer.v <- function(contingency.tab) {\n",
    "#     chi <-  chisq.test(contingency.tab, correct = FALSE)$statistic\n",
    "#     n <- sum(contingency.tab)\n",
    "#     c <- min(nrow(contingency.tab), ncol(contingency.tab))\n",
    "#     V <- sqrt(chi / (n * (c-1)))\n",
    "#     return(as.numeric(V))\n",
    "# }\n",
    "\n",
    "## https://www.datascienceblog.net/post/statistical_test/effect_size/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical modeling for hypothesis testing <a class=\"anchor\" id=\"statistical_modeling_for_hypothesis_testing\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most problems that we considered above can be viewed from two perspectives: one is making a statistical hypothesis and verify or falsify that hypothesis (as in the examples above), the other one is **make a statistical model and analyze the significance of the model parameters**. This second perspective is the subject of this section. \n",
    "\n",
    "Book: An Introduction to Statistics with Python (p.144, p.207)\\\n",
    "http://psych.colorado.edu/~carey/Courses/PSYC5741/handouts/GLM%20Theory.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 1: Linear Regression for Two-sample Student's t-test <a class=\"anchor\" id=\"linear_regression_t_test\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the **two sample Student's t-test** we want **compare two groups of observations**. The goal is to determine if both groups could have been generated by the **same process with the same mean**. It assumes populations with normal distribution $N(\\mu_{1}, \\sigma_{1})$ and $N(\\mu_{2}, \\sigma_{2})$.\n",
    "\n",
    "Here, instead of calculating directly the t-statistic from the observed data, we perform a **linear regression**, where the independent variable represents the groups and the dependent variable represents the numerical variable we are trying to analize.\n",
    "If we write the regression line (i.e. the best fit) as\\\n",
    "$\\hat{y_i} = \\hat{y}(x_i) = \\theta_1 + x_i \\theta_2$ \\\n",
    "then the $y_i$ values can be written as\\\n",
    "$y_i = \\theta_1 + x_i \\theta_2 + \\epsilon_i$ \\\n",
    "where the error $\\epsilon_i$ represents the part of y not explained by the linear model (residuals)\\\n",
    "$\\epsilon_i = y_i - \\hat{y_i}$\\\n",
    "Here the **independent variable $x_{i}$ takes only two values (*dummy variable*), for example $x_i=0,1$ that identify group 1 and group 2**, and the dependent variable $y_i$ represents the numerical variable. \n",
    "\n",
    "We can calculate explicitely the value of $\\theta_1$ and $\\theta_2$ in this specific case where we have just two $x_i$ values. We want to minimize the residuals sum of squares $\\sum_{i}^{n}\\epsilon_i^2$, i.e. $\\frac{\\partial \\sum_{i}^{n}\\epsilon_i^2}{\\partial \\theta_1}=0$ and $\\frac{\\partial \\sum_{i}^{n}\\epsilon_i^2}{\\partial \\theta_2}=0$.\\\n",
    "$\\frac{\\partial}{\\partial \\theta_1}\\sum_{i}^{n}(y_i-\\theta_1 - x_i \\theta_2)^2=\\frac{\\partial}{\\partial \\theta_1}(\\sum_{j}^{n_1}(y_j-\\theta_1 - x_1 \\theta_2)^2+\\sum_{k}^{n_2}(y_k-\\theta_1 - x_2 \\theta_2)^2)=-2\\sum_{j}^{n_1}(y_j-\\theta_1 - x_1 \\theta_2)-2\\sum_{k}^{n_2}(y_k-\\theta_1 - x_2 \\theta_2)=-2(\\sum_{j}^{n_1}y_j-n_1\\theta_1 - n_1 x_1 \\theta_2+\\sum_{k}^{n_2}y_k-n_2\\theta_1 - n_2 x_2 \\theta_2)=-2(n_1\\bar{y_1}+n_2\\bar{y_2}-\\theta_1(n_1+n_2)-\\theta_2(n_1x_1+n_2x_2))=0$\\\n",
    "$\\frac{\\partial}{\\partial \\theta_2}\\sum_{i}^{n}(y_i-\\theta_1 - x_i \\theta_2)^2=\\frac{\\partial}{\\partial \\theta_2}(\\sum_{j}^{n_1}(y_j-\\theta_1 - x_1 \\theta_2)^2+\\sum_{k}^{n_2}(y_k-\\theta_1 - x_2 \\theta_2)^2)=-2x_1\\sum_{j}^{n_1}(y_j-\\theta_1 - x_1 \\theta_2)-2x_2\\sum_{k}^{n_2}(y_k-\\theta_1 - x_2 \\theta_2)=-2(x_1\\sum_{j}^{n_1}y_j-n_1x_1\\theta_1 - n_1 x_1^2 \\theta_2+x_2\\sum_{k}^{n_2}y_k-n_2x_2\\theta_1 - n_2 x_2^2 \\theta_2)=-2(n_1x_1\\bar{y_1}+n_2x_2\\bar{y_2}-\\theta_1(n_1x_1+n_2x_2)-\\theta_2(n_1x_1^2+n_2x_2^2))=0$\n",
    "\n",
    "For $x_1=0$ and $x_2=1$, we get\\\n",
    "$n_1\\bar{y_1}+n_2\\bar{y_2}-\\theta_1(n_1+n_2)-n_2\\theta_2=0$ \\\n",
    "$\\bar{y_2}-\\theta_1-\\theta_2=0$\\\n",
    "which gives\\\n",
    "$\\theta_1 =\\bar{y_1} = \\frac{\\sum_{j}^{n_1}y_j}{n_1}$ \\\n",
    "$\\theta_2 =\\bar{y_2}-\\bar{y_1} = \\frac{\\sum_{k}^{n_2}y_k}{n_2}-\\frac{\\sum_{j}^{n_1}y_j}{n_1}$\n",
    "\n",
    "So, for $x$ taking only two values $x_i=0,1$, we have that <b> $\\theta_1$ corresponds to the mean of  group 1 </b>, and <b>$\\theta_2$ corresponds to the difference between the mean of the group 2 and the mean of the group 1</b> (ref 1, 2 and 3). \n",
    "\n",
    "Therefore we can write the **t-statistics** as\\\n",
    "$t = \\frac{\\bar{y}_{2}-\\bar{y}_{1}}{s_{\\bar{\\Delta}}} = \\frac{\\theta_2}{s_{\\bar{\\Delta}}}$ \\\n",
    "where $s_{\\bar{\\Delta}}^{2}=\\frac{s^{2}_{1}}{n1}+\\frac{s^{2}_{2}}{n2}$, $s^{2}_{1}=\\frac{\\sum_{j}^{n1}(y_{j}- {{\\bar{y}_{1}}})^2}{n1-1}$ and $s^{2}_{2}=\\frac{\\sum_{k}^{n_2}(y_{k}- {{\\bar{y}_{2}}})^2}{n2-1}$ are the variances of groups 1 and 2.\n",
    "\n",
    "In this context, the ASSUMPTION OF NORMALITY translates in assuming that the $\\epsilon_i \\sim N(0,\\sigma_{\\epsilon}^{2})$.\n",
    "\n",
    "CHECK IT!!! The alternative hypothesis assumes that some difference exists between the two mean values $\\mu_{1} \\neq \\mu_{2}$, whereas the null hypothesis assumes that no difference exists $\\mu_{1}=\\mu_{2}$.\n",
    "\n",
    "<ol>\n",
    "<li> We have two random samples: $n_{1}$ elements $y_{j}$ taken from group 1 and $n_{2}$ elements $y_{k}$ taken from group 2. </li>\n",
    "<li> Null hypothesis $H_{0}: \\mu_{1}=\\mu_{2}$.</li>\n",
    "<li> From the observed sample data we perform a linear regression and we calculate the <b> test-statistic\n",
    "    $t = \\frac{\\theta_2}{s_{\\bar{\\Delta}}}$ </b>. If we assume the null hypothesis, i.e. the two samples are randomly taken from normal distributions with the same mean, then the statistic <b>t can be approximated as an ordinary Student's t-distribution with the degrees of freedom calculated using: check Welch's t-test</b>.   \n",
    "</li>\n",
    "<li> <b>Comparing the observed value of the statistics with the corresponding distribution</b>, we can find the likelihood that a value as extreme as or more than the observed one is found by chance. This is the <b>p-value</b>.</li>\n",
    "<li> If <b>p < 0.05</b> (or below a threshold we decide), we <b>reject the null hypothesis</b> and speak of a statistically significant difference. </li>\n",
    "</ol>  \n",
    "\n",
    "<br />\n",
    "The two samples t-test has these main assumptions:\n",
    "\n",
    "- normally distributed\n",
    "- independent groups\n",
    "\n",
    "1 - C.Sala Notes \\\n",
    "2 - https://stats.stackexchange.com/questions/59047/how-are-regression-the-t-test-and-the-anova-all-versions-of-the-general-linear \\\n",
    "3 - Book: An Introduction to Statistics with Python (p.202, p.207)\n",
    "4 - http://seismo.berkeley.edu/~kirchner/eps_120/Toolkits/Toolkit_10.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example X: Generalised Linear Model for Hypothesis Testing: TODO <a class=\"anchor\" id=\"glm_t_test\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Numerical Example**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’re going to model Poisson Regression related to how frequently yarn breaks during weaving. The data set looks at how many warp breaks occurred for different types of looms, per fixed length of yarn.\n",
    "\n",
    "**TODO: not clear interpretation of the three models**\n",
    "\n",
    "https://www.dataquest.io/blog/tutorial-poisson-regression-in-r/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(datasets)\n",
    "data <- warpbreaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>breaks</th><th scope=col>wool</th><th scope=col>tension</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>26</td><td>A </td><td>L </td></tr>\n",
       "\t<tr><td>30</td><td>A </td><td>L </td></tr>\n",
       "\t<tr><td>54</td><td>A </td><td>L </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lll}\n",
       " breaks & wool & tension\\\\\n",
       "\\hline\n",
       "\t 26 & A  & L \\\\\n",
       "\t 30 & A  & L \\\\\n",
       "\t 54 & A  & L \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| breaks | wool | tension |\n",
       "|---|---|---|\n",
       "| 26 | A  | L  |\n",
       "| 30 | A  | L  |\n",
       "| 54 | A  | L  |\n",
       "\n"
      ],
      "text/plain": [
       "  breaks wool tension\n",
       "1 26     A    L      \n",
       "2 30     A    L      \n",
       "3 54     A    L      "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "     breaks      wool   tension\n",
       " Min.   :10.00   A:27   L:18   \n",
       " 1st Qu.:18.25   B:27   M:18   \n",
       " Median :26.00          H:18   \n",
       " Mean   :28.15                 \n",
       " 3rd Qu.:34.00                 \n",
       " Max.   :70.00                 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "head(data,3)\n",
    "summary(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeAAAAEsCAMAAAAsIJBoAAAAFVBMVEUAAAAzMzNNTU3r6+v/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",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "options(repr.plot.width=4, repr.plot.height=2.5)\n",
    "\n",
    "pp1 <- ggplot(data,aes(x=breaks))+geom_histogram(col=\"red\",fill=\"#ff9999\", binwidth=5)+\n",
    "theme(axis.text=element_text(size=8),axis.title=element_text(size=10))+ ylab(\"counts\")+xlab(\"# of warp breaks\")+\n",
    "ggtitle(\"\")\n",
    "pp1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "28.1481481481481"
      ],
      "text/latex": [
       "28.1481481481481"
      ],
      "text/markdown": [
       "28.1481481481481"
      ],
      "text/plain": [
       "[1] 28.14815"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "174.204053109713"
      ],
      "text/latex": [
       "174.204053109713"
      ],
      "text/markdown": [
       "174.204053109713"
      ],
      "text/plain": [
       "[1] 174.2041"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Check if it's poissonian distribution: variance is equal to the mean\n",
    "mean(data$breaks)\n",
    "var(data$breaks)\n",
    "# The variance is much greater than the mean, which suggests that we will have over-dispersion in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "glm(formula = breaks ~ wool + tension, family = poisson(link = \"log\"), \n",
       "    data = data)\n",
       "\n",
       "Deviance Residuals: \n",
       "    Min       1Q   Median       3Q      Max  \n",
       "-3.6871  -1.6503  -0.4269   1.1902   4.2616  \n",
       "\n",
       "Coefficients:\n",
       "            Estimate Std. Error z value Pr(>|z|)    \n",
       "(Intercept)  3.69196    0.04541  81.302  < 2e-16 ***\n",
       "woolB       -0.20599    0.05157  -3.994 6.49e-05 ***\n",
       "tensionM    -0.32132    0.06027  -5.332 9.73e-08 ***\n",
       "tensionH    -0.51849    0.06396  -8.107 5.21e-16 ***\n",
       "---\n",
       "Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n",
       "\n",
       "(Dispersion parameter for poisson family taken to be 1)\n",
       "\n",
       "    Null deviance: 297.37  on 53  degrees of freedom\n",
       "Residual deviance: 210.39  on 50  degrees of freedom\n",
       "AIC: 493.06\n",
       "\n",
       "Number of Fisher Scoring iterations: 4\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Let's try to fit with poissonian glm\n",
    "poisson.model <- glm(breaks ~ wool + tension, data, family = poisson(link = \"log\"))\n",
    "summary(poisson.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general rule is that if there are k categories in a factor variable, the output of glm() will have k−1 categories with remaining 1 as the base category.\n",
    "In our case $A$ has been made the base and is not shown in summary. Similarly, for tension $L$ has been made the base category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the Residual Deviance is greater than the degrees of freedom, then over-dispersion exists. This means that the estimates are correct, but the standard errors (standard deviation) are wrong and unaccounted for by the model.\n",
    "\n",
    "https://www.dataquest.io/blog/tutorial-poisson-regression-in-r/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "glm(formula = breaks ~ wool + tension, family = quasipoisson(link = \"log\"), \n",
       "    data = data)\n",
       "\n",
       "Deviance Residuals: \n",
       "    Min       1Q   Median       3Q      Max  \n",
       "-3.6871  -1.6503  -0.4269   1.1902   4.2616  \n",
       "\n",
       "Coefficients:\n",
       "            Estimate Std. Error t value Pr(>|t|)    \n",
       "(Intercept)  3.69196    0.09374  39.384  < 2e-16 ***\n",
       "woolB       -0.20599    0.10646  -1.935 0.058673 .  \n",
       "tensionM    -0.32132    0.12441  -2.583 0.012775 *  \n",
       "tensionH    -0.51849    0.13203  -3.927 0.000264 ***\n",
       "---\n",
       "Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n",
       "\n",
       "(Dispersion parameter for quasipoisson family taken to be 4.261537)\n",
       "\n",
       "    Null deviance: 297.37  on 53  degrees of freedom\n",
       "Residual deviance: 210.39  on 50  degrees of freedom\n",
       "AIC: NA\n",
       "\n",
       "Number of Fisher Scoring iterations: 4\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Let's try to fit with quasi-poisson model\n",
    "poisson.model2 <- glm(breaks ~ wool + tension, data = data, family = quasipoisson(link = \"log\"))\n",
    "summary(poisson.model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "glm.nb(formula = breaks ~ wool + tension, data = data, init.theta = 9.944385436, \n",
       "    link = log)\n",
       "\n",
       "Deviance Residuals: \n",
       "    Min       1Q   Median       3Q      Max  \n",
       "-2.0144  -0.9319  -0.2240   0.5828   1.8220  \n",
       "\n",
       "Coefficients:\n",
       "            Estimate Std. Error z value Pr(>|z|)    \n",
       "(Intercept)   3.6734     0.0979  37.520  < 2e-16 ***\n",
       "woolB        -0.1862     0.1010  -1.844   0.0651 .  \n",
       "tensionM     -0.2992     0.1217  -2.458   0.0140 *  \n",
       "tensionH     -0.5114     0.1237  -4.133 3.58e-05 ***\n",
       "---\n",
       "Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n",
       "\n",
       "(Dispersion parameter for Negative Binomial(9.9444) family taken to be 1)\n",
       "\n",
       "    Null deviance: 75.464  on 53  degrees of freedom\n",
       "Residual deviance: 53.723  on 50  degrees of freedom\n",
       "AIC: 408.76\n",
       "\n",
       "Number of Fisher Scoring iterations: 1\n",
       "\n",
       "\n",
       "              Theta:  9.94 \n",
       "          Std. Err.:  2.56 \n",
       "\n",
       " 2 x log-likelihood:  -398.764 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Let's try to fit with negative binomial model\n",
    "library(MASS)\n",
    "nb.model <- glm.nb(breaks ~ wool + tension, data = data)\n",
    "summary(nb.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's calculate the power of test.**\n",
    "\n",
    "The numerator degrees of freedom, u, is the number of coefficients you'll have in your model (minus the intercept). The denominator degrees of freedom, v, is the number of error degrees of freedom: v = n - u - 1. This implies n = v + u + 1. \\\n",
    "f2, is $\\frac{R^{2}}{1 - R^{2}}$, where $R^{2}$ is the coefficient of determination.\n",
    "\n",
    "https://cran.r-project.org/web/packages/pwr/vignettes/pwr-vignette.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(rsq)\n",
    "model = nb.model\n",
    "\n",
    "R2 <- rsq(model) # https://rdrr.io/cran/rsq/man/rsq.html\n",
    "f2 <- R2/(1-R2)\n",
    "\n",
    "n <- dim(data)[1]\n",
    "u <- length(nb.model$coefficients)-1\n",
    "v <- n-u-1\n",
    "sig_level <- 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "     Multiple regression power calculation \n",
       "\n",
       "              u = 3\n",
       "              v = 50\n",
       "             f2 = 0.4038135\n",
       "      sig.level = 0.05\n",
       "          power = 0.9759659\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "library(pwr)\n",
    "pwr.f2.test(u = u, f2 = f2, sig.level = sig_level, v = v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "54"
      ],
      "text/latex": [
       "54"
      ],
      "text/markdown": [
       "54"
      ],
      "text/plain": [
       "[1] 54"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a 98% chance of correctly rejecting the null hypothesis, assuming that the magnitude of the effect observed is true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "     Multiple regression power calculation \n",
       "\n",
       "              u = 3\n",
       "              v = 27.15989\n",
       "             f2 = 0.4043764\n",
       "      sig.level = 0.05\n",
       "          power = 0.8\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pwr.f2.test(u = u, f2 = f2, sig.level = sig_level, power=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would have needed only $n = v+u+1 = 31$ total observations to correctly reject the null hypothesis with a 80% probability (assuming the effect size we observe is true)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example XX: Linear Regression for One-way ANOVA (balanced): TODO <a class=\"anchor\" id=\"linear_regression_anova\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 5 groups with n number of elements each one\n",
    "\n",
    "If we write the regression line (i.e. the best fit) as\\\n",
    "$\\hat{y_i} = \\hat{y}(x_i) = \\theta_1 + x_i \\theta_2$ \\\n",
    "then the $y_i$ values can be written as\\\n",
    "$y_i = \\theta_1 + x_i \\theta_2 + \\epsilon_i$ \\\n",
    "Here the **independent variable $x_{i}$ takes five values (*dummy variable*), for example $x_i=0,1,2,3,4$ that identify group 1 to 5**, and the dependent variable $y_i$ represents the numerical variable. \n",
    "\n",
    "We can calculate explicitely the value of $\\theta_1$ and $\\theta_2$ in this specific case where we have five $x_i$ values. We want to minimize the residuals sum of squares $\\sum_{i}^{n}\\epsilon_i^2$, i.e. $\\frac{\\partial \\sum_{i}^{n}\\epsilon_i^2}{\\partial \\theta_1}=0$ and $\\frac{\\partial \\sum_{i}^{n}\\epsilon_i^2}{\\partial \\theta_2}=0$.\n",
    "\n",
    "$\\frac{\\partial}{\\partial \\theta_1}\\sum_{i}^{n_{tot}}(y_i-\\theta_1 - x_i \\theta_2)^2=\\frac{\\partial}{\\partial \\theta_1}(\\sum_{i}^{n}(y_i-\\theta_1 - x_1 \\theta_2)^2+\\sum_{j}^{n}(y_j-\\theta_1 - x_2 \\theta_2)^2+ ...+\\sum_{k}^{n}(y_k-\\theta_1 - x_5 \\theta_2)^2)= -2n(\\bar{y}_1-\\theta_1-x_1 \\theta_2)-\\dots - 2n(\\bar{y}_5-\\theta_1-x_5 \\theta_2)= -2n(\\bar{y}_1+\\dots+\\bar{y}_5-5\\theta_1 - \\theta_2(x_1 + \\dots + x_5))=0$\\\n",
    "$\\frac{\\partial}{\\partial \\theta_2}\\sum_{i}^{n}(y_i-\\theta_1 - x_i \\theta_2)^2= \\frac{\\partial}{\\partial \\theta_2}(\\sum_{i}^{n}(y_i-\\theta_1 - x_1 \\theta_2)^2+\\sum_{j}^{n}(y_j-\\theta_1 - x_2 \\theta_2)^2+ ...+\\sum_{k}^{n}(y_k-\\theta_1 - x_5 \\theta_2)^2)=-2nx_1(\\bar{y}_1-\\theta_1-x_1 \\theta_2)-\\dots - 2nx_5(\\bar{y}_5-\\theta_1-x_5 \\theta_2)=\n",
    "-2n(x_1\\bar{y}_1+ \\dots +x_5 \\bar{y}_5 - \\theta_1(x_1 + \\dots + x_5)-\\theta_2 (x_1^2+ \\dots + x_5^2))=\n",
    "0$\n",
    "\n",
    "For $x_i=0,1,2,3,4$\n",
    "\n",
    "$\\bar{y}_1+\\dots+\\bar{y}_5-5\\theta_1 - 10\\theta_2=0$\\\n",
    "$x_1\\bar{y}_1+ \\dots +x_5 \\bar{y}_5 - \\theta_1(x_1 + \\dots + x_5)-\\theta_2 (x_1^2+ \\dots + x_5^2)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confidence Interval <a class=\"anchor\" id=\"confidence_interval\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **confidence interval (CI) is a type of estimate computed from the statistics (for example mean, std) of the observed data**. This proposes a range of plausible values for an unknown parameter (for example, the mean). The interval has an associated confidence level that the true parameter is in the proposed range. The $\\alpha$% confidence interval reports the range that contains the true value for the parameter with a likelihood of $\\alpha$%.\n",
    "\n",
    "This means that the confidence level represents the theoretical long-run frequency (i.e., the proportion) of confidence intervals that contain the true value of the unknown population parameter. In other words, 90% of confidence intervals computed at the 90% confidence level contain the parameter, 95% of confidence intervals computed at the 95% confidence level contain the parameter, 99% of confidence intervals computed at the 99% confidence level contain the parameter.\n",
    "\n",
    "If you choose a population with a fixed mean, collect sample data, and finally calculate the 95% confidence interval, 95% of the time the interval you calculated will cover the true mean. Once you’ve calculated a confidence interval, it’s incorrect to say that it covers the true mean with a probability of 95% (this is a common misinterpretation). You can only say, in advance, that in the long-run, 95% of the confidence intervals you’ve generated by following the same procedure will cover the true mean.\n",
    "\n",
    "wiki - Confidence interval \\\n",
    "Book: An Introduction to Statistics with Python \\\n",
    "https://www.probabilisticworld.com/frequentist-bayesian-approaches-inferential-statistics/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
